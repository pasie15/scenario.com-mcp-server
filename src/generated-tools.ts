
import { Tool } from "@modelcontextprotocol/sdk/types.js";
import axios from "axios";
import * as dotenv from "dotenv";

dotenv.config();

const BASE_URL = "https://api.cloud.scenario.com/v1";

function getAuthHeaders() {
    const key = process.env.SCENARIO_API_KEY;
    const secret = process.env.SCENARIO_API_SECRET;
    if (!key || !secret) {
        throw new Error("Missing SCENARIO_API_KEY or SCENARIO_API_SECRET");
    }
    const authString = Buffer.from(`${key}:${secret}`).toString('base64');
    return {
        "Authorization": `Basic ${authString}`,
        "Content-Type": "application/json"
    };
}

export const tools: Tool[] = [
    {
        "name": "get-assets",
        "description": "List assets of a project team, or list all public assets",
        "inputSchema": {
            "type": "object",
            "properties": {
                "updatedBefore": {
                    "type": "string",
                    "description": "Filter results to only return assets updated before the specified ISO string date (exclusive). Requires the sortBy parameter to be \"updatedAt\""
                },
                "sortDirection": {
                    "type": "string",
                    "description": "Sort results in ascending (asc) or descending (desc) order"
                },
                "privacy": {
                    "type": "string",
                    "description": "Filters result by asset privacy. If set to public, it will return -all- public assets from all organizations"
                },
                "inferenceId": {
                    "type": "string",
                    "description": "List assets generated from a specific inference"
                },
                "modelId": {
                    "type": "string",
                    "description": "List assets generated from all inferences coming from a specific model (this is not the training images)"
                },
                "updatedAfter": {
                    "type": "string",
                    "description": "Filter results to only return assets updated after the specified ISO string date (exclusive). Requires the sortBy parameter to be \"updatedAt\""
                },
                "parentAssetId": {
                    "type": "string",
                    "description": "List all the children assets that were generated from a specific parent asset"
                },
                "createdBefore": {
                    "type": "string",
                    "description": "Filter results to only return assets created before the specified ISO string date (exclusive). Requires the sortBy parameter to be \"createdAt\""
                },
                "sortBy": {
                    "type": "string",
                    "description": "Sort results by the createdAt or updatedAt"
                },
                "createdAfter": {
                    "type": "string",
                    "description": "Filter results to only return assets created after the specified ISO string date (exclusive). Requires the sortBy parameter to be \"createdAt\""
                },
                "authorId": {
                    "type": "string",
                    "description": "List assets generated by a specific author (the user that created the asset)"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 50, maximum value is 100, minimum value is 1"
                },
                "rootAssetId": {
                    "type": "string",
                    "description": "List all the children assets that were generated from a specific root asset"
                },
                "type": {
                    "type": "string",
                    "description": "List all the assets of a specific type. The parameter \"type\" and \"types\" cannot be used together. Can be any of the following values: inference-txt2img, inference-txt2img-ip-adapter, inference-txt2img-texture, inference-img2img, inference-img2img-ip-adapter, inference-img2img-texture, inference-inpaint, inference-inpaint-ip-adapter, inference-reference, inference-reference-texture, inference-controlnet, inference-controlnet-ip-adapter, inference-controlnet-img2img, inference-controlnet-reference, inference-controlnet-inpaint, inference-controlnet-inpaint-ip-adapter, inference-controlnet-texture, background-removal, canvas, canvas-export, canvas-drawing, detection, patch, pixelization, upscale, upscale-texture, upscale-skybox, vectorization, segment, segmentation-image, segmentation-mask, skybox-base-360, skybox-hdri, skybox-3d, restyle, reframe, generative-fill, texture, texture-height, texture-normal, texture-smoothness, texture-metallic, texture-edge, texture-ao, texture-albedo, image-prompt-editing, unknown, img23d, txt23d, 3d23d, 3d23d-texture, img2video, txt2audio, audio2audio, video2video, txt2img, img2img, txt2video, uploaded, uploaded-video, uploaded-audio, uploaded-3d, uploaded-avatar, upscale-video, assets with a type starting with \"inference-\" will be returned",
                    "enum": [
                        "3d23d",
                        "3d23d-texture",
                        "audio2audio",
                        "background-removal",
                        "canvas",
                        "canvas-drawing",
                        "canvas-export",
                        "detection",
                        "generative-fill",
                        "image-prompt-editing",
                        "img23d",
                        "img2img",
                        "img2video",
                        "inference-controlnet",
                        "inference-controlnet-img2img",
                        "inference-controlnet-inpaint",
                        "inference-controlnet-inpaint-ip-adapter",
                        "inference-controlnet-ip-adapter",
                        "inference-controlnet-reference",
                        "inference-controlnet-texture",
                        "inference-img2img",
                        "inference-img2img-ip-adapter",
                        "inference-img2img-texture",
                        "inference-inpaint",
                        "inference-inpaint-ip-adapter",
                        "inference-reference",
                        "inference-reference-texture",
                        "inference-txt2img",
                        "inference-txt2img-ip-adapter",
                        "inference-txt2img-texture",
                        "patch",
                        "pixelization",
                        "reframe",
                        "restyle",
                        "segment",
                        "segmentation-image",
                        "segmentation-mask",
                        "skybox-3d",
                        "skybox-base-360",
                        "skybox-hdri",
                        "texture",
                        "texture-albedo",
                        "texture-ao",
                        "texture-edge",
                        "texture-height",
                        "texture-metallic",
                        "texture-normal",
                        "texture-smoothness",
                        "txt23d",
                        "txt2audio",
                        "txt2img",
                        "txt2video",
                        "unknown",
                        "uploaded",
                        "uploaded-3d",
                        "uploaded-audio",
                        "uploaded-avatar",
                        "uploaded-video",
                        "upscale",
                        "upscale-skybox",
                        "upscale-texture",
                        "upscale-video",
                        "vectorization",
                        "video2video"
                    ]
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                },
                "tags": {
                    "type": "string",
                    "description": "List of tags, comma separated. Only for public assets on all teams."
                },
                "types": {
                    "type": "array",
                    "items": {
                        "type": "string",
                        "enum": [
                            "3d23d",
                            "3d23d-texture",
                            "audio2audio",
                            "background-removal",
                            "canvas",
                            "canvas-drawing",
                            "canvas-export",
                            "detection",
                            "generative-fill",
                            "image-prompt-editing",
                            "img23d",
                            "img2img",
                            "img2video",
                            "inference-controlnet",
                            "inference-controlnet-img2img",
                            "inference-controlnet-inpaint",
                            "inference-controlnet-inpaint-ip-adapter",
                            "inference-controlnet-ip-adapter",
                            "inference-controlnet-reference",
                            "inference-controlnet-texture",
                            "inference-img2img",
                            "inference-img2img-ip-adapter",
                            "inference-img2img-texture",
                            "inference-inpaint",
                            "inference-inpaint-ip-adapter",
                            "inference-reference",
                            "inference-reference-texture",
                            "inference-txt2img",
                            "inference-txt2img-ip-adapter",
                            "inference-txt2img-texture",
                            "patch",
                            "pixelization",
                            "reframe",
                            "restyle",
                            "segment",
                            "segmentation-image",
                            "segmentation-mask",
                            "skybox-3d",
                            "skybox-base-360",
                            "skybox-hdri",
                            "texture",
                            "texture-albedo",
                            "texture-ao",
                            "texture-edge",
                            "texture-height",
                            "texture-metallic",
                            "texture-normal",
                            "texture-smoothness",
                            "txt23d",
                            "txt2audio",
                            "txt2img",
                            "txt2video",
                            "unknown",
                            "uploaded",
                            "uploaded-3d",
                            "uploaded-audio",
                            "uploaded-avatar",
                            "uploaded-video",
                            "upscale",
                            "upscale-skybox",
                            "upscale-texture",
                            "upscale-video",
                            "vectorization",
                            "video2video"
                        ]
                    }
                },
                "collectionId": {
                    "type": "string"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-asset",
        "description": "Upload an image or canvas",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "image": {
                    "type": "string",
                    "description": "The image to upload in base64 format string."
                },
                "canvas": {
                    "type": "string",
                    "description": "The canvas to upload as a stringified JSON. Ignored if `image` is provided."
                },
                "thumbnail": {
                    "type": "string",
                    "description": "The thumbnail for the canvas in base64 format string. Ignored if `image` is provided."
                },
                "hide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the asset."
                },
                "collectionIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "The original file name of the image (example: \"low-res-image.jpg\")."
                },
                "parentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the asset."
                }
            },
            "required": [
                "name"
            ]
        }
    },
    {
        "name": "delete-asset",
        "description": "Delete multiple assets",
        "inputSchema": {
            "type": "object",
            "properties": {
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "assetIds"
            ]
        }
    },
    {
        "name": "post-download-assets",
        "description": "Request a link to batch download assets (batch limited to 1000 assets)",
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "object",
                    "properties": {
                        "modelIds": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "inferenceIds": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "assetIds": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        }
                    },
                    "required": [
                        "assetIds",
                        "inferenceIds",
                        "modelIds"
                    ]
                },
                "options": {
                    "type": "object",
                    "properties": {
                        "fileNameTemplate": {
                            "type": "string",
                            "description": "A file naming convention as a string with the following available parameters:\n<seed> (seed used to generate the asset)\n<num> (index of the asset in the inference)\n<prompt> (prompt of the inference)\n<generator> (prompt of the generator)\nExample: \"<generator>-<prompt>-<num>-<seed>\""
                        },
                        "flat": {
                            "type": "boolean",
                            "description": "Flag to prevent grouping assets in directories and store them flat"
                        }
                    },
                    "required": [
                        "fileNameTemplate"
                    ]
                }
            },
            "required": [
                "query",
                "options"
            ]
        }
    },
    {
        "name": "get-download-assets",
        "description": "Retrieve the status and the url of a batch download assets request",
        "inputSchema": {
            "type": "object",
            "properties": {
                "jobId": {
                    "type": "string",
                    "description": "The job ID to retrieve the download request"
                }
            },
            "required": [
                "jobId"
            ]
        }
    },
    {
        "name": "post-asset-get-bulk",
        "description": "Get multiple assets by their IDs",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": []
        }
    },
    {
        "name": "get-public-assets",
        "description": "List all public assets",
        "inputSchema": {
            "type": "object",
            "properties": {
                "updatedBefore": {
                    "type": "string",
                    "description": "Filter results to only return assets updated before the specified ISO string date (exclusive). Requires the sortBy parameter to be \"updatedAt\""
                },
                "sortDirection": {
                    "type": "string",
                    "description": "Sort results in ascending (asc) or descending (desc) order"
                },
                "modelId": {
                    "type": "string",
                    "description": "List assets generated from all inferences coming from a specific model (this is not the training images)"
                },
                "updatedAfter": {
                    "type": "string",
                    "description": "Filter results to only return assets updated after the specified ISO string date (exclusive). Requires the sortBy parameter to be \"updatedAt\""
                },
                "createdBefore": {
                    "type": "string",
                    "description": "Filter results to only return assets created before the specified ISO string date (exclusive). Requires the sortBy parameter to be \"createdAt\""
                },
                "sortBy": {
                    "type": "string",
                    "description": "Sort results by the createdAt or updatedAt"
                },
                "createdAfter": {
                    "type": "string",
                    "description": "Filter results to only return assets created after the specified ISO string date (exclusive). Requires the sortBy parameter to be \"createdAt\""
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 50, maximum value is 100, minimum value is 1"
                },
                "type": {
                    "type": "string",
                    "description": "List all the assets of a specific type. The parameter \"type\" and \"types\" cannot be used together. Can be any of the following values: inference-txt2img, inference-txt2img-ip-adapter, inference-txt2img-texture, inference-img2img, inference-img2img-ip-adapter, inference-img2img-texture, inference-inpaint, inference-inpaint-ip-adapter, inference-reference, inference-reference-texture, inference-controlnet, inference-controlnet-ip-adapter, inference-controlnet-img2img, inference-controlnet-reference, inference-controlnet-inpaint, inference-controlnet-inpaint-ip-adapter, inference-controlnet-texture, background-removal, canvas, canvas-export, canvas-drawing, detection, patch, pixelization, upscale, upscale-texture, upscale-skybox, vectorization, segment, segmentation-image, segmentation-mask, skybox-base-360, skybox-hdri, skybox-3d, restyle, reframe, generative-fill, texture, texture-height, texture-normal, texture-smoothness, texture-metallic, texture-edge, texture-ao, texture-albedo, image-prompt-editing, unknown, img23d, txt23d, 3d23d, 3d23d-texture, img2video, txt2audio, audio2audio, video2video, txt2img, img2img, txt2video, uploaded, uploaded-video, uploaded-audio, uploaded-3d, uploaded-avatar, upscale-video, assets with a type starting with \"inference-\" will be returned"
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                },
                "tags": {
                    "type": "string",
                    "description": "List of tags, comma separated. Only for public assets on all teams."
                },
                "types": {
                    "type": "string",
                    "description": "List of the asset types to request. The parameter \"type\" and \"types\" cannot be used together. Can be any of the following values: inference-txt2img, inference-txt2img-ip-adapter, inference-txt2img-texture, inference-img2img, inference-img2img-ip-adapter, inference-img2img-texture, inference-inpaint, inference-inpaint-ip-adapter, inference-reference, inference-reference-texture, inference-controlnet, inference-controlnet-ip-adapter, inference-controlnet-img2img, inference-controlnet-reference, inference-controlnet-inpaint, inference-controlnet-inpaint-ip-adapter, inference-controlnet-texture, background-removal, canvas, canvas-export, canvas-drawing, detection, patch, pixelization, upscale, upscale-texture, upscale-skybox, vectorization, segment, segmentation-image, segmentation-mask, skybox-base-360, skybox-hdri, skybox-3d, restyle, reframe, generative-fill, texture, texture-height, texture-normal, texture-smoothness, texture-metallic, texture-edge, texture-ao, texture-albedo, image-prompt-editing, unknown, img23d, txt23d, 3d23d, 3d23d-texture, img2video, txt2audio, audio2audio, video2video, txt2img, img2img, txt2video, uploaded, uploaded-video, uploaded-audio, uploaded-3d, uploaded-avatar, upscale-video"
                }
            },
            "required": []
        }
    },
    {
        "name": "get-public-assets-by-asset-id",
        "description": "Get the details of an asset",
        "inputSchema": {
            "type": "object",
            "properties": {
                "assetId": {
                    "type": "string",
                    "description": "The asset ID to retrieve"
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "get-assets-by-asset-id",
        "description": "Get the details of an asset",
        "inputSchema": {
            "type": "object",
            "properties": {
                "withEmbedding": {
                    "type": "string",
                    "description": "Include the embedding in the response"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "assetId": {
                    "type": "string",
                    "description": "The asset ID to retrieve"
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "put-asset-by-asset-id",
        "description": "Update a canvas asset",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "assetId": {
                    "type": "string"
                },
                "lockId": {
                    "type": "string",
                    "description": "The value of the lock to use when updating a locked canvas."
                },
                "lockExpiresAt": {
                    "type": "string",
                    "description": "The ISO timestamp when the lock on the canvas will expire."
                },
                "canvas": {
                    "type": "string",
                    "description": "The new value for the canvas as a stringified JSON."
                },
                "thumbnail": {
                    "type": "string",
                    "description": "The new thumbnail for the canvas in base64 format string."
                },
                "name": {
                    "type": "string",
                    "description": "The new name for the canvas."
                },
                "description": {
                    "type": "string",
                    "description": "The new description of the asset."
                },
                "disableSnapshot": {
                    "type": "boolean",
                    "description": "If true, no snapshot will be created for this update."
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "copy-asset-by-asset-id",
        "description": "Duplicate an asset",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "assetId": {
                    "type": "string",
                    "description": "The ID of the asset to duplicate"
                },
                "targetProjectId": {
                    "type": "string",
                    "description": "The id of the project to copy the asset to. If not provided, the asset will be copied to the canvas project."
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "lock-asset-by-asset-id",
        "description": "Lock a canvas",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "assetId": {
                    "type": "string",
                    "description": "The ID of the canvas to lock"
                },
                "lockExpiresAt": {
                    "type": "string",
                    "description": "The ISO timestamp when the lock on the canvas will expire."
                }
            },
            "required": [
                "assetId",
                "lockExpiresAt"
            ]
        }
    },
    {
        "name": "get-canvas-asset-snapshots",
        "description": "List snapshots of a canvas type asset",
        "inputSchema": {
            "type": "object",
            "properties": {
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 10, maximum value is 100, minimum value is 10"
                },
                "assetId": {
                    "type": "string",
                    "description": "The ID of the canvas asset to list snapshots for"
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "put-assets-tags-by-asset-id",
        "description": "Add/delete tags on a specific asset",
        "inputSchema": {
            "type": "object",
            "properties": {
                "assetId": {
                    "type": "string",
                    "description": "The ID of the asset to update its tags"
                },
                "add": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "strict": {
                    "type": "boolean",
                    "description": "If true, the function will throw an error if:\n- one of the tags to add already exists\n- one of the tags to delete is not found\nIf false, the endpoint will behave as if it was idempotent"
                },
                "delete": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "unlock-asset-by-asset-id",
        "description": "Unlock a canvas",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "assetId": {
                    "type": "string",
                    "description": "The ID of the canvas to unlock"
                },
                "lockId": {
                    "type": "string",
                    "description": "The value of the lock on this canvas."
                },
                "forceUnlock": {
                    "type": "boolean",
                    "description": "If true, no need to pass a lockId."
                }
            },
            "required": [
                "assetId"
            ]
        }
    },
    {
        "name": "get-collections",
        "description": "List collections of a team",
        "inputSchema": {
            "type": "object",
            "properties": {
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 10, maximum value is 100, minimum value is 1"
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-collection",
        "description": "Create a new collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the collection."
                }
            },
            "required": [
                "name"
            ]
        }
    },
    {
        "name": "get-collections-by-collection-id",
        "description": "Get the details of a collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The collection ID to retrieve"
                }
            },
            "required": [
                "collectionId"
            ]
        }
    },
    {
        "name": "put-collections-by-collection-id",
        "description": "Update the name of a Collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The collectionId to update"
                },
                "name": {
                    "type": "string",
                    "description": "The new name for the Collection"
                }
            },
            "required": [
                "collectionId",
                "name"
            ]
        }
    },
    {
        "name": "delete-collections-by-collection-id",
        "description": "Delete a collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The collection ID to delete"
                }
            },
            "required": [
                "collectionId"
            ]
        }
    },
    {
        "name": "put-assets-by-collection-id",
        "description": "Add assets to a specific collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The ID of the collection to add assets to"
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "collectionId",
                "assetIds"
            ]
        }
    },
    {
        "name": "delete-assets-by-collection-id",
        "description": "Remove assets from a specific collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The ID of the collection to remove the assets from"
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "collectionId",
                "assetIds"
            ]
        }
    },
    {
        "name": "put-models-by-collection-id",
        "description": "Add models to a specific collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The ID of the collection to add models to"
                },
                "modelIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "collectionId",
                "modelIds"
            ]
        }
    },
    {
        "name": "delete-models-by-collection-id",
        "description": "Remove models from a specific collection",
        "inputSchema": {
            "type": "object",
            "properties": {
                "collectionId": {
                    "type": "string",
                    "description": "The ID of the collection to remove the models from"
                },
                "modelIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "collectionId",
                "modelIds"
            ]
        }
    },
    {
        "name": "post-caption-inferences",
        "description": "Caption image(s)",
        "inputSchema": {
            "type": "object",
            "properties": {
                "dryRun": {
                    "type": "string"
                },
                "ensureIPCleared": {
                    "type": "boolean",
                    "description": "Whether we try to ensure IP removal for new prompt generation."
                },
                "images": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "number",
                    "description": "If specified, the API will make a best effort to produce the same results, such that repeated requests with the same `seed` and parameters should return the same outputs. Must be used along with the same parameters including prompt, model's state, etc.."
                },
                "unwantedSequences": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "modelId": {
                    "type": "string",
                    "description": "When provided, the model will follow the model's training images and examples' prompt to generate the captions."
                },
                "temperature": {
                    "type": "number",
                    "description": "The sampling temperature to use. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic.\n\nWe generally recommend altering this or `topP` but not both."
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "topP": {
                    "type": "number",
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So `0.1` means only the tokens comprising the top `10%` probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both."
                },
                "detailsLevel": {
                    "type": "string",
                    "description": "The details level used to generate the captions.\n\nWhen a modelId is provided and examples are available, the details level is ignored.",
                    "enum": [
                        "action",
                        "action+style"
                    ]
                }
            },
            "required": [
                "images"
            ]
        }
    },
    {
        "name": "post-controlnet-inferences",
        "description": "Trigger a new image generation in ControlNet mode. The control image is used to guide the generation; it can be a pose, canny map, or similar.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "controlEnd": {
                    "type": "number",
                    "description": "Specifies how long the ControlNet guidance should be applied during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance is active.\nFor example:\n- 1.0: ControlNet guidance is applied during all inference steps\n- 0.5: ControlNet guidance is only applied during the first half of inference steps\n\nDefault values:\n- 0.5 for Canny modality\n- 0.6 for all other modalities"
                },
                "modality": {
                    "type": "string",
                    "description": "The modality associated with the control image used for the generation: it can either be an object with a combination of maximum\n\nFor models of SD1.5 family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `lines`, `seg`, `scribble`, `lineart`, `normal-map`, `illusion`\n - or one of the following presets: `character`, `landscape`, `city`, `interior`.\n\nFor models of the SDXL family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `seg`, `illusion`, `scribble`\n - or one of the following presets: `character`, `landscape`.\n\nFor models of the FLUX schnell or dev families:\n- one modality from: `canny`, `tile`, `depth`, `blur`, `pose`, `gray`, `low-quality`\n\nOptionally, you can associate a value to these modalities or presets. The value must be within `]0.0, 1.0]`.\n\nExamples:\n- `canny`\n- `depth:0.5,pose:1.0`\n- `canny:0.5,depth:0.5,lines:0.3`\n- `landscape`\n- `character:0.5`\n- `illusion:1`\n\nNote: if you use a value that is not supported by the model family, this will result in an error.",
                    "enum": [
                        "blur",
                        "canny",
                        "depth",
                        "gray",
                        "illusion",
                        "lineart",
                        "lines",
                        "low-quality",
                        "normal-map",
                        "pose",
                        "scribble",
                        "seg",
                        "sketch",
                        "tile"
                    ]
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableModalityDetection": {
                    "type": "boolean",
                    "description": "If false, the process uses the given image to detect the modality.\nIf true (default), the process will not try to detect the modality of the given image.\n\nFor example:\nwith `pose` modality and `false` value, the process will detect the pose of people in the given image\nwith `depth` modality and `false` value, the process will detect the depth of the given image\nwith `scribble` modality and `true`value, the process will use the given image as a scribble\n\n⚠️ For models of the FLUX schnell or dev families, this parameter is ignored. The modality detection is always disabled. ⚠️"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "controlStart": {
                    "type": "number",
                    "description": "Specifies the starting point of the ControlNet guidance during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance starts.\nFor example:\n- 0.0: ControlNet guidance starts at the beginning of the inference steps\n- 0.5: ControlNet guidance starts at the middle of the inference steps"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "controlImageId": {
                    "type": "string",
                    "description": "The controlnet input image as an AssetId. Will be ignored if the `controlnet` parameter is provided"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "controlImage": {
                    "type": "string",
                    "description": "The controlnet input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modality",
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-controlnet-img2img-inferences",
        "description": "Trigger a new image generation in ControlNet + Img2Img mode. The control image is used to guide the generation; it can be a pose, canny map, or similar. The reference image is used to initialize the generation process.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "controlEnd": {
                    "type": "number",
                    "description": "Specifies how long the ControlNet guidance should be applied during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance is active.\nFor example:\n- 1.0: ControlNet guidance is applied during all inference steps\n- 0.5: ControlNet guidance is only applied during the first half of inference steps\n\nDefault values:\n- 0.5 for Canny modality\n- 0.6 for all other modalities"
                },
                "modality": {
                    "type": "string",
                    "description": "The modality associated with the control image used for the generation: it can either be an object with a combination of maximum\n\nFor models of SD1.5 family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `lines`, `seg`, `scribble`, `lineart`, `normal-map`, `illusion`\n - or one of the following presets: `character`, `landscape`, `city`, `interior`.\n\nFor models of the SDXL family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `seg`, `illusion`, `scribble`\n - or one of the following presets: `character`, `landscape`.\n\nFor models of the FLUX schnell or dev families:\n- one modality from: `canny`, `tile`, `depth`, `blur`, `pose`, `gray`, `low-quality`\n\nOptionally, you can associate a value to these modalities or presets. The value must be within `]0.0, 1.0]`.\n\nExamples:\n- `canny`\n- `depth:0.5,pose:1.0`\n- `canny:0.5,depth:0.5,lines:0.3`\n- `landscape`\n- `character:0.5`\n- `illusion:1`\n\nNote: if you use a value that is not supported by the model family, this will result in an error.",
                    "enum": [
                        "blur",
                        "canny",
                        "depth",
                        "gray",
                        "illusion",
                        "lineart",
                        "lines",
                        "low-quality",
                        "normal-map",
                        "pose",
                        "scribble",
                        "seg",
                        "sketch",
                        "tile"
                    ]
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableModalityDetection": {
                    "type": "boolean",
                    "description": "If false, the process uses the given image to detect the modality.\nIf true (default), the process will not try to detect the modality of the given image.\n\nFor example:\nwith `pose` modality and `false` value, the process will detect the pose of people in the given image\nwith `depth` modality and `false` value, the process will detect the depth of the given image\nwith `scribble` modality and `true`value, the process will use the given image as a scribble\n\n⚠️ For models of the FLUX schnell or dev families, this parameter is ignored. The modality detection is always disabled. ⚠️"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "controlStart": {
                    "type": "number",
                    "description": "Specifies the starting point of the ControlNet guidance during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance starts.\nFor example:\n- 0.0: ControlNet guidance starts at the beginning of the inference steps\n- 0.5: ControlNet guidance starts at the middle of the inference steps"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "controlImageId": {
                    "type": "string",
                    "description": "The controlnet input image as an AssetId. Will be ignored if the `controlnet` parameter is provided"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "controlImage": {
                    "type": "string",
                    "description": "The controlnet input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modality",
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-controlnet-inpaint-inferences",
        "description": "Trigger a new image generation in ControlNet + Inpaint mode. The control image is used to guide the generation; it can be a pose, canny map, or similar. The mask indicates the area to inpaint in the reference image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "controlEnd": {
                    "type": "number",
                    "description": "Specifies how long the ControlNet guidance should be applied during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance is active.\nFor example:\n- 1.0: ControlNet guidance is applied during all inference steps\n- 0.5: ControlNet guidance is only applied during the first half of inference steps\n\nDefault values:\n- 0.5 for Canny modality\n- 0.6 for all other modalities"
                },
                "modality": {
                    "type": "string",
                    "description": "The modality associated with the control image used for the generation: it can either be an object with a combination of maximum\n\nFor models of SD1.5 family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `lines`, `seg`, `scribble`, `lineart`, `normal-map`, `illusion`\n - or one of the following presets: `character`, `landscape`, `city`, `interior`.\n\nFor models of the SDXL family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `seg`, `illusion`, `scribble`\n - or one of the following presets: `character`, `landscape`.\n\nFor models of the FLUX schnell or dev families:\n- one modality from: `canny`, `tile`, `depth`, `blur`, `pose`, `gray`, `low-quality`\n\nOptionally, you can associate a value to these modalities or presets. The value must be within `]0.0, 1.0]`.\n\nExamples:\n- `canny`\n- `depth:0.5,pose:1.0`\n- `canny:0.5,depth:0.5,lines:0.3`\n- `landscape`\n- `character:0.5`\n- `illusion:1`\n\nNote: if you use a value that is not supported by the model family, this will result in an error.",
                    "enum": [
                        "blur",
                        "canny",
                        "depth",
                        "gray",
                        "illusion",
                        "lineart",
                        "lines",
                        "low-quality",
                        "normal-map",
                        "pose",
                        "scribble",
                        "seg",
                        "sketch",
                        "tile"
                    ]
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableMerging": {
                    "type": "boolean",
                    "description": "If set to true, the entire input image will likely change during inpainting. This results in faster inferences, but the output image will be harder to integrate if the input is just a small part of a larger image."
                },
                "disableModalityDetection": {
                    "type": "boolean",
                    "description": "If false, the process uses the given image to detect the modality.\nIf true (default), the process will not try to detect the modality of the given image.\n\nFor example:\nwith `pose` modality and `false` value, the process will detect the pose of people in the given image\nwith `depth` modality and `false` value, the process will detect the depth of the given image\nwith `scribble` modality and `true`value, the process will use the given image as a scribble\n\n⚠️ For models of the FLUX schnell or dev families, this parameter is ignored. The modality detection is always disabled. ⚠️"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "controlStart": {
                    "type": "number",
                    "description": "Specifies the starting point of the ControlNet guidance during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance starts.\nFor example:\n- 0.0: ControlNet guidance starts at the beginning of the inference steps\n- 0.5: ControlNet guidance starts at the middle of the inference steps"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                },
                "controlImageId": {
                    "type": "string",
                    "description": "The controlnet input image as an AssetId. Will be ignored if the `controlnet` parameter is provided"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId. Will be ignored if the `image` parameter is provided"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "controlImage": {
                    "type": "string",
                    "description": "The controlnet input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modality",
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-controlnet-inpaint-ip-adapter-inferences",
        "description": "Trigger a new image generation in ControlNet + Inpaint + IpAdapter mode. The control image is used to guide the generation; it can be a pose, canny map, or similar. The mask indicates the area to inpaint in the reference image, and the second reference image is used as an IPAdapter to guide the generation process.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "ipAdapterImageIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "ipAdapterType": {
                    "type": "string",
                    "description": "The type of IP Adapter model to use. Must be one of [`style`, `character`], default to `style``",
                    "enum": [
                        "character",
                        "style"
                    ]
                },
                "ipAdapterImage": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImages` instead.\nThe IpAdapter image as a data url. Will be ignored if the `ipAdapterImages` parameter is provided."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableMerging": {
                    "type": "boolean",
                    "description": "If set to true, the entire input image will likely change during inpainting. This results in faster inferences, but the output image will be harder to integrate if the input is just a small part of a larger image."
                },
                "ipAdapterImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                },
                "controlImageId": {
                    "type": "string",
                    "description": "The controlnet input image as an AssetId. Will be ignored if the `controlnet` parameter is provided"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "ipAdapterImageId": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImageIds` instead.\nThe IpAdapter image as an AssetId. Cannot be set if `ipAdapterImage` is provided. Will be ignored if the `ipAdapterImageIds` parameter is provided."
                },
                "ipAdapterScale": {
                    "type": "number",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterScales` instead.\nIpAdapter scale factor (within [0.0, 1.0], default: 0.9). Will be ignored if the `ipAdapterScales` parameter is provided"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "ipAdapterScales": {
                    "type": "array",
                    "items": {
                        "type": "number"
                    }
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId. Will be ignored if the `image` parameter is provided"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "controlImage": {
                    "type": "string",
                    "description": "The controlnet input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-controlnet-ip-adapter-inferences",
        "description": "Trigger a new image generation in ControlNet + IpAdapter mode. The control image is used to guide the generation; it can be a pose, canny map, or similar. The second reference image is used as an IPAdapter to guide the generation process.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "ipAdapterImageIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "controlEnd": {
                    "type": "number",
                    "description": "Specifies how long the ControlNet guidance should be applied during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance is active.\nFor example:\n- 1.0: ControlNet guidance is applied during all inference steps\n- 0.5: ControlNet guidance is only applied during the first half of inference steps\n\nDefault values:\n- 0.5 for Canny modality\n- 0.6 for all other modalities"
                },
                "modality": {
                    "type": "string",
                    "description": "The modality associated with the control image used for the generation: it can either be an object with a combination of maximum\n\nFor models of SD1.5 family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `lines`, `seg`, `scribble`, `lineart`, `normal-map`, `illusion`\n - or one of the following presets: `character`, `landscape`, `city`, `interior`.\n\nFor models of the SDXL family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `seg`, `illusion`, `scribble`\n - or one of the following presets: `character`, `landscape`.\n\nFor models of the FLUX schnell or dev families:\n- one modality from: `canny`, `tile`, `depth`, `blur`, `pose`, `gray`, `low-quality`\n\nOptionally, you can associate a value to these modalities or presets. The value must be within `]0.0, 1.0]`.\n\nExamples:\n- `canny`\n- `depth:0.5,pose:1.0`\n- `canny:0.5,depth:0.5,lines:0.3`\n- `landscape`\n- `character:0.5`\n- `illusion:1`\n\nNote: if you use a value that is not supported by the model family, this will result in an error.",
                    "enum": [
                        "blur",
                        "canny",
                        "depth",
                        "gray",
                        "illusion",
                        "lineart",
                        "lines",
                        "low-quality",
                        "normal-map",
                        "pose",
                        "scribble",
                        "seg",
                        "sketch",
                        "tile"
                    ]
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "ipAdapterType": {
                    "type": "string",
                    "description": "The type of IP Adapter model to use. Must be one of [`style`, `character`], default to `style``",
                    "enum": [
                        "character",
                        "style"
                    ]
                },
                "ipAdapterImage": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImages` instead.\nThe IpAdapter image as a data url. Will be ignored if the `ipAdapterImages` parameter is provided."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableModalityDetection": {
                    "type": "boolean",
                    "description": "If false, the process uses the given image to detect the modality.\nIf true (default), the process will not try to detect the modality of the given image.\n\nFor example:\nwith `pose` modality and `false` value, the process will detect the pose of people in the given image\nwith `depth` modality and `false` value, the process will detect the depth of the given image\nwith `scribble` modality and `true`value, the process will use the given image as a scribble\n\n⚠️ For models of the FLUX schnell or dev families, this parameter is ignored. The modality detection is always disabled. ⚠️"
                },
                "ipAdapterImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "controlStart": {
                    "type": "number",
                    "description": "Specifies the starting point of the ControlNet guidance during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance starts.\nFor example:\n- 0.0: ControlNet guidance starts at the beginning of the inference steps\n- 0.5: ControlNet guidance starts at the middle of the inference steps"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "controlImageId": {
                    "type": "string",
                    "description": "The controlnet input image as an AssetId. Will be ignored if the `controlnet` parameter is provided"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "ipAdapterImageId": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImageIds` instead.\nThe IpAdapter image as an AssetId. Cannot be set if `ipAdapterImage` is provided. Will be ignored if the `ipAdapterImageIds` parameter is provided."
                },
                "ipAdapterScale": {
                    "type": "number",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterScales` instead.\nIpAdapter scale factor (within [0.0, 1.0], default: 0.9). Will be ignored if the `ipAdapterScales` parameter is provided"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "ipAdapterScales": {
                    "type": "array",
                    "items": {
                        "type": "number"
                    }
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "controlImage": {
                    "type": "string",
                    "description": "The controlnet input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modality",
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-controlnet-texture-inferences",
        "description": "Trigger a new seamless texture image generation in ControlNet mode. The control image is used to guide the generation; it can be a pose, canny map, or similar.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "controlEnd": {
                    "type": "number",
                    "description": "Specifies how long the ControlNet guidance should be applied during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance is active.\nFor example:\n- 1.0: ControlNet guidance is applied during all inference steps\n- 0.5: ControlNet guidance is only applied during the first half of inference steps\n\nDefault values:\n- 0.5 for Canny modality\n- 0.6 for all other modalities"
                },
                "modality": {
                    "type": "string",
                    "description": "The modality associated with the control image used for the generation: it can either be an object with a combination of maximum\n\nFor models of SD1.5 family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `lines`, `seg`, `scribble`, `lineart`, `normal-map`, `illusion`\n - or one of the following presets: `character`, `landscape`, `city`, `interior`.\n\nFor models of the SDXL family:\n - up to 3 modalities from `canny`, `pose`, `depth`, `seg`, `illusion`, `scribble`\n - or one of the following presets: `character`, `landscape`.\n\nFor models of the FLUX schnell or dev families:\n- one modality from: `canny`, `tile`, `depth`, `blur`, `pose`, `gray`, `low-quality`\n\nOptionally, you can associate a value to these modalities or presets. The value must be within `]0.0, 1.0]`.\n\nExamples:\n- `canny`\n- `depth:0.5,pose:1.0`\n- `canny:0.5,depth:0.5,lines:0.3`\n- `landscape`\n- `character:0.5`\n- `illusion:1`\n\nNote: if you use a value that is not supported by the model family, this will result in an error.",
                    "enum": [
                        "blur",
                        "canny",
                        "depth",
                        "gray",
                        "illusion",
                        "lineart",
                        "lines",
                        "low-quality",
                        "normal-map",
                        "pose",
                        "scribble",
                        "seg",
                        "sketch",
                        "tile"
                    ]
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableModalityDetection": {
                    "type": "boolean",
                    "description": "If false, the process uses the given image to detect the modality.\nIf true (default), the process will not try to detect the modality of the given image.\n\nFor example:\nwith `pose` modality and `false` value, the process will detect the pose of people in the given image\nwith `depth` modality and `false` value, the process will detect the depth of the given image\nwith `scribble` modality and `true`value, the process will use the given image as a scribble\n\n⚠️ For models of the FLUX schnell or dev families, this parameter is ignored. The modality detection is always disabled. ⚠️"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "controlStart": {
                    "type": "number",
                    "description": "Specifies the starting point of the ControlNet guidance during the inference process.\n\nOnly available for Flux.1-dev based models.\n\nThe value represents the percentage of total inference steps where the ControlNet guidance starts.\nFor example:\n- 0.0: ControlNet guidance starts at the beginning of the inference steps\n- 0.5: ControlNet guidance starts at the middle of the inference steps"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "controlImageId": {
                    "type": "string",
                    "description": "The controlnet input image as an AssetId. Will be ignored if the `controlnet` parameter is provided"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "controlImage": {
                    "type": "string",
                    "description": "The controlnet input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modality",
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-generate-custom",
        "description": "Generate with any model (Image, Video, Audio, 3d).\n\nYou can retrieve the model inputs from the `GET /models/{modelId}` endpoint.\n\n**Note**: This endpoint is not available yet for SD1.5, SDXL, Flux.1 and Flux.1-Kontext based models. For these models, use the `POST /generate/{inferenceType}` endpoint. Ex: `POST /generate/txt2img` or `POST /generate/prompt-editing`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "dryRun": {
                    "type": "string"
                },
                "modelId": {
                    "type": "string"
                },
                "body": {
                    "type": "string",
                    "description": "The request body for the custom generation must be retrieve from GET /models/{modelId} inputs fields"
                }
            },
            "required": [
                "modelId",
                "body"
            ]
        }
    },
    {
        "name": "post-describe-style-inferences",
        "description": "Describe the style of the given images or models.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "dryRun": {
                    "type": "string"
                },
                "ensureIPCleared": {
                    "type": "boolean",
                    "description": "Whether we try to ensure IP removal for new prompt generation."
                },
                "images": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "number",
                    "description": "If specified, the API will make a best effort to produce the same results, such that repeated requests with the same `seed` and parameters should return the same outputs. Must be used along with the same parameters including prompt, model's state, etc.."
                },
                "unwantedSequences": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "modelId": {
                    "type": "string",
                    "description": "The modelId used to condition the generation.\n\nWhen provided, the generation will take into account model's training images, examples.\n\nIn `contextual` mode, the modelId is used to retrieve additional context from the model such as its type and capabilities."
                },
                "temperature": {
                    "type": "number",
                    "description": "The sampling temperature to use. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic.\n\nWe generally recommend altering this or `topP` but not both."
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "topP": {
                    "type": "number",
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So `0.1` means only the tokens comprising the top `10%` probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both."
                }
            },
            "required": []
        }
    },
    {
        "name": "post-detect-inferences",
        "description": "Advanced precision in image generation by transforming visual data from input images into mode maps.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The image to be used to detect. Must reference an existing AssetId or be a data URL."
                },
                "modality": {
                    "type": "string",
                    "description": "Modality to detect",
                    "enum": [
                        "canny",
                        "depth",
                        "grayscale",
                        "lineart_anime",
                        "mlsd",
                        "normal",
                        "pose",
                        "scribble",
                        "segmentation",
                        "sketch"
                    ]
                },
                "lowThreshold": {
                    "type": "number",
                    "description": "Low threshold for Canny detector"
                },
                "removeBackground": {
                    "type": "boolean",
                    "description": "Remove background for Grayscale detector"
                },
                "minThreshold": {
                    "type": "number",
                    "description": "Minimum threshold for Grayscale conversion"
                },
                "maxThreshold": {
                    "type": "number",
                    "description": "Maximum threshold for Grayscale conversion"
                },
                "factor": {
                    "type": "number",
                    "description": "Contrast factor for Grayscale detector"
                },
                "highThreshold": {
                    "type": "number",
                    "description": "High threshold for Canny detector"
                },
                "keypointThreshold": {
                    "type": "number",
                    "description": "How polished is the surface? 0 is like a rough surface, 1 is like a mirror"
                }
            },
            "required": [
                "image",
                "modality"
            ]
        }
    },
    {
        "name": "post-embed-inferences",
        "description": "Get embeddings from text",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "text": {
                    "type": "string",
                    "description": "The text to embed. Must be a non-empty string."
                }
            },
            "required": [
                "text"
            ]
        }
    },
    {
        "name": "post-generative-fill-inferences",
        "description": "Generative fill replace the selected mask area content based on the context. Used to erase objects or characters.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The image from which the mask will be refilled generatively. Must reference an existing AssetId or be a data URL."
                },
                "targetHeight": {
                    "type": "number",
                    "description": "The target height of the output image."
                },
                "seed": {
                    "type": "number",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation."
                },
                "promptFidelity": {
                    "type": "number",
                    "description": "Increase the fidelity to the prompt during the restyle."
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId, used to determine the area to refill generatively. Will be ignored if the `mask` parameter is provided. Must reference an existing AssetId."
                },
                "prompt": {
                    "type": "string",
                    "description": "A full text prompt to guide the repaint process."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "A negative full text prompt that discourages the repaint from generating certain characteristics. It is recommended to test without using a negative prompt."
                },
                "targetWidth": {
                    "type": "number",
                    "description": "The target width of the output image."
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "post-img2img-inferences",
        "description": "Trigger a new image generation in Img2Img mode with one reference image that initializes the generation process.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-img2img-ip-adapter-inferences",
        "description": "Trigger a new image generation in Img2Img + IpAdapter mode. The first image is used to initialize the generation, and the second reference image is used as an IPAdapter.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "ipAdapterImageIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "ipAdapterType": {
                    "type": "string",
                    "description": "The type of IP Adapter model to use. Must be one of [`style`, `character`], default to `style``",
                    "enum": [
                        "character",
                        "style"
                    ]
                },
                "ipAdapterImage": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImages` instead.\nThe IpAdapter image as a data url. Will be ignored if the `ipAdapterImages` parameter is provided."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "ipAdapterImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "ipAdapterImageId": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImageIds` instead.\nThe IpAdapter image as an AssetId. Cannot be set if `ipAdapterImage` is provided. Will be ignored if the `ipAdapterImageIds` parameter is provided."
                },
                "ipAdapterScale": {
                    "type": "number",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterScales` instead.\nIpAdapter scale factor (within [0.0, 1.0], default: 0.9). Will be ignored if the `ipAdapterScales` parameter is provided"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "ipAdapterScales": {
                    "type": "array",
                    "items": {
                        "type": "number"
                    }
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId. Will be ignored if the `image` parameter is provided"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-img2img-texture-inferences",
        "description": "Trigger a new seamless texture image generation in Img2Img mode with one reference image that initializes the generation.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId. Will be ignored if the `image` parameter is provided"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-inpaint-inferences",
        "description": "Trigger a new image generation in Inpaint mode. The mask indicates the area to inpaint in the reference image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId. Will be ignored if the `image` parameter is provided"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "disableMerging": {
                    "type": "boolean",
                    "description": "If set to true, the entire input image will likely change during inpainting. This results in faster inferences, but the output image will be harder to integrate if the input is just a small part of a larger image."
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-inpaint-ip-adapter-inferences",
        "description": "Trigger a new image generation in Inpaint + IpAdapter mode. The mask indicates the area to inpaint in the reference image, and the second reference image is used as an IPAdapter to guide the inpainting.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "ipAdapterImageIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "ipAdapterType": {
                    "type": "string",
                    "description": "The type of IP Adapter model to use. Must be one of [`style`, `character`], default to `style``",
                    "enum": [
                        "character",
                        "style"
                    ]
                },
                "ipAdapterImage": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImages` instead.\nThe IpAdapter image as a data url. Will be ignored if the `ipAdapterImages` parameter is provided."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "disableMerging": {
                    "type": "boolean",
                    "description": "If set to true, the entire input image will likely change during inpainting. This results in faster inferences, but the output image will be harder to integrate if the input is just a small part of a larger image."
                },
                "ipAdapterImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "imageParentId": {
                    "type": "string",
                    "description": "Specifies the parent asset Id for the image when provided as a dataurl."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "imageHide": {
                    "type": "boolean",
                    "description": "Toggles the hidden status of the image when provided as a dataurl."
                },
                "mask": {
                    "type": "string",
                    "description": "The mask as a data URL, used to determine the area of change. The mask is a binary mask made out of white and black pixels. The white area is the one that will be replaced. (example: \"data:image/png;base64,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\")"
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "imageId": {
                    "type": "string",
                    "description": "Deprecated: The input image as an AssetId. Prefer to use image with the asset ID instead."
                },
                "ipAdapterImageId": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImageIds` instead.\nThe IpAdapter image as an AssetId. Cannot be set if `ipAdapterImage` is provided. Will be ignored if the `ipAdapterImageIds` parameter is provided."
                },
                "ipAdapterScale": {
                    "type": "number",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterScales` instead.\nIpAdapter scale factor (within [0.0, 1.0], default: 0.9). Will be ignored if the `ipAdapterScales` parameter is provided"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "ipAdapterScales": {
                    "type": "array",
                    "items": {
                        "type": "number"
                    }
                },
                "maskId": {
                    "type": "string",
                    "description": "The mask as an AssetId. Will be ignored if the `image` parameter is provided"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-patch-inferences",
        "description": "Patch an asset with an image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "patch": {
                    "type": "object",
                    "properties": {
                        "mode": {
                            "type": "string",
                            "description": "The mode of merging the images: `override` or `erase`.",
                            "enum": [
                                "erase",
                                "override"
                            ]
                        },
                        "image": {
                            "type": "string",
                            "description": "The source of the image to be merged, as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                        }
                    },
                    "required": [
                        "image",
                        "mode"
                    ]
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")"
                },
                "backgroundColor": {
                    "type": "string",
                    "description": "The background color as an hexadecimal code (ex: \"#FFFFFF\"), an html color (ex: \"red\") or \"transparent\" if \"format\" is \"png\". Default to \"white\""
                },
                "format": {
                    "type": "string",
                    "description": "The output format. Default to \"png\"",
                    "enum": [
                        "jpeg",
                        "png"
                    ]
                },
                "position": {
                    "type": "object",
                    "properties": {
                        "x": {
                            "type": "number",
                            "description": "The X position of the image to be merged, in pixels."
                        },
                        "y": {
                            "type": "number",
                            "description": "The Y position of the image to be merged, in pixels."
                        }
                    },
                    "required": [
                        "x",
                        "y"
                    ]
                },
                "allowOverflow": {
                    "type": "boolean",
                    "description": "Whether to allow the merged image to extend the size of the original (when x or y are negative or merged image is bigger)"
                },
                "crop": {
                    "type": "object",
                    "properties": {
                        "backgroundColor": {
                            "type": "string"
                        },
                        "top": {
                            "type": "number"
                        },
                        "left": {
                            "type": "number"
                        },
                        "width": {
                            "type": "number"
                        },
                        "height": {
                            "type": "number"
                        }
                    },
                    "required": [
                        "height",
                        "left",
                        "top",
                        "width"
                    ]
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "post-pixelate-inferences",
        "description": "Advanced pixelization of an image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\") to pixelate."
                },
                "pixelGridSize": {
                    "type": "number",
                    "description": "The size of the pixel grid in the output image. Should be 16, 32, 64, 128, or 256."
                },
                "removeNoise": {
                    "type": "boolean",
                    "description": "Reduce pixel art artifacts."
                },
                "removeBackground": {
                    "type": "boolean",
                    "description": "Remove the background from the image."
                },
                "colorPalette": {
                    "type": "array",
                    "items": {
                        "type": "array",
                        "items": {
                            "type": "number"
                        }
                    }
                },
                "colorPaletteSize": {
                    "type": "number",
                    "description": "If no colorPalette is provided, you can provide a palette size. Value should be between 2 and 256."
                }
            },
            "required": [
                "image",
                "pixelGridSize",
                "removeNoise"
            ]
        }
    },
    {
        "name": "post-prompt-inferences",
        "description": "Generate, complete or invent new prompts.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "dryRun": {
                    "type": "string"
                },
                "mode": {
                    "type": "string",
                    "description": "The mode used to generate new prompt(s).",
                    "enum": [
                        "completion",
                        "contextual",
                        "image-editing",
                        "inventive",
                        "structured"
                    ]
                },
                "ensureIPCleared": {
                    "type": "boolean",
                    "description": "Whether we try to ensure IP removal for new prompt generation."
                },
                "image": {
                    "type": "string",
                    "description": "The input image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\") or the asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\")\n\nRequired when `mode` is `image-editing-prompt`."
                },
                "images": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "number",
                    "description": "If specified, the API will make a best effort to produce the same results, such that repeated requests with the same `seed` and parameters should return the same outputs. Must be used along with the same parameters including prompt, model's state, etc.."
                },
                "modelId": {
                    "type": "string",
                    "description": "The modelId used to condition the generation.\n\nWhen provided, the generation will take into account model's training images, examples.\n\nOnly supports 'gemini-2.0-flash', 'gemini-2.5-flash', 'gpt-image-1', 'flux-kontext' and 'runway-gen4-image' for now when `mode` is `image-editing-prompt`."
                },
                "temperature": {
                    "type": "number",
                    "description": "The sampling temperature to use. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic.\n\nWe generally recommend altering this or `topP` but not both."
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "numResults": {
                    "type": "number",
                    "description": "The number of results to return."
                },
                "prompt": {
                    "type": "string",
                    "description": "The initial prompt spark feed to `completion`, `inventive` or `structured` modes."
                },
                "topP": {
                    "type": "number",
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So `0.1` means only the tokens comprising the top `10%` probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both."
                }
            },
            "required": [
                "mode"
            ]
        }
    },
    {
        "name": "post-prompt-editing-inferences",
        "description": "Edit an image with a prompt.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The image to edit. Must reference an existing AssetId or be a data URL."
                },
                "referenceImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "number",
                    "description": "The seed to use for the image generation.\n\nOnly available for the `flux-kontext` model."
                },
                "strength": {
                    "type": "number",
                    "description": "The strength\n\nOnly available for the `flux-kontext` LoRA model."
                },
                "guidanceScale": {
                    "type": "number",
                    "description": "The guidance scale to use for the image generation.\n\nOnly available for the `flux-kontext` model."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model to use. Can be \"gemini-2.0-flash\", \"gemini-2.5-flash\", \"gpt-image-1\", \"flux-kontext\", \"runway-gen4-image\" or \"seedream-4\".",
                    "enum": [
                        "flux-kontext",
                        "gemini-2.0-flash",
                        "gemini-2.5-flash",
                        "gpt-image-1",
                        "runway-gen4-image",
                        "seedream-4"
                    ]
                },
                "format": {
                    "type": "string",
                    "description": "The format of the generated image(s)\n\nThis parameter is only supported for the `gpt-image-1` model.",
                    "enum": [
                        "jpeg",
                        "png",
                        "webp"
                    ]
                },
                "aspectRatio": {
                    "type": "string",
                    "description": "The aspect ratio of the generated image(s).\nSupported for: `gemini-2.5-flash`, `gpt-image-1`, `flux-kontext`, `runway-gen4-image`, `seedream-4`.\nWill default to `auto` for other models and unknown ratios.\n\nNotes:\n- `gemini-2.5-flash` supports Landscape: 21:9, 16:9, 4:3, 3:2 • Square: 1:1 • Portrait: 9:16, 3:4, 2:3 • Flexible: 5:4, 4:5 • `auto`.\n- `gpt-image-1` supports `1:1`, `3:2`, `2:3`, `auto` (unknown ratios fall back to `auto`).\n- `runway-gen4-image` supports `1:1`, `4:3`, `3:4`, `16:9`, `9:16`, `auto` (unknown ratios fall back to `auto`).\n- `seedream-4` supports `1:1`, `4:3`, `3:4`, `16:9`, `9:16`, `2:3`, `3:2`, `21:9`, `auto`.",
                    "enum": [
                        "16:9",
                        "1:1",
                        "21:9",
                        "2:3",
                        "3:2",
                        "3:4",
                        "4:3",
                        "4:5",
                        "5:4",
                        "9:16",
                        "9:21",
                        "auto"
                    ]
                },
                "inputFidelity": {
                    "type": "string",
                    "description": "When set to `high`, allows to better preserve details from the input images in the output.\nThis is especially useful when using images that contain elements like faces or logos that\nrequire accurate preservation in the generated image.\n\nYou can provide multiple input images that will all be preserved with high fidelity, but keep\nin mind that the first image will be preserved with richer textures and finer details, so if\nyou include elements such as faces, consider placing them in the first image.\n\nOnly available for the `gpt-image-1` model.",
                    "enum": [
                        "high",
                        "low"
                    ]
                },
                "quality": {
                    "type": "string",
                    "description": "The quality of the generated image(s).\n\nOnly available for the `gpt-image-1`, `flux-kontext`, `runway-gen4-image` and `seedream-4` models.",
                    "enum": [
                        "high",
                        "low",
                        "medium"
                    ]
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of samples to generate\n\nMaximum depends on the subscription tier."
                },
                "compression": {
                    "type": "number",
                    "description": "The compression level (0-100%) for the generated images. This parameter is only supported for\nthe `gpt-image-1` model with the `webp` or `jpeg` output formats, and defaults to 100."
                },
                "prompt": {
                    "type": "string",
                    "description": "The prompt to edit the given image."
                },
                "mask": {
                    "type": "string",
                    "description": "You can provide a mask to indicate where the image should be edited. The black area of the mask\nwill be replaced, while the filled areas will be kept as is.\n\nMust reference an existing AssetId or be a data URL.\n\nOnly available for the `gpt-image-1` model. Will be ignored for other models."
                }
            },
            "required": [
                "image",
                "prompt"
            ]
        }
    },
    {
        "name": "post-reframe-inferences",
        "description": "Reframe a given image to new sizes. Extra space is filled based on the context.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The image to reframe. Must reference an existing AssetId or be a data URL."
                },
                "inputLocation": {
                    "type": "string",
                    "description": "Location of the input image in the output.",
                    "enum": [
                        "bottom",
                        "left",
                        "middle",
                        "right",
                        "top"
                    ]
                },
                "seed": {
                    "type": "number",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "horizontalExpansionRatio": {
                    "type": "number",
                    "description": "(deprecated) Horizontal expansion ratio."
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "(deprecated) A negative full text prompt that discourages the repaint from generating certain characteristics. It is recommended to test without using a negative prompt."
                },
                "resizeOption": {
                    "type": "number",
                    "description": "Size proportion of the input image in the output."
                },
                "verticalExpansionRatio": {
                    "type": "number",
                    "description": "(deprecated) Vertical expansion ratio."
                },
                "targetHeight": {
                    "type": "number",
                    "description": "The target height of the output image."
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation."
                },
                "promptFidelity": {
                    "type": "number",
                    "description": "Increase the fidelity to the prompt during the restyle."
                },
                "overlapPercentage": {
                    "type": "number",
                    "description": "Overlap percentage for the output image."
                },
                "prompt": {
                    "type": "string",
                    "description": "A full text prompt to guide the repaint process."
                },
                "targetWidth": {
                    "type": "number",
                    "description": "The target width of the output image."
                }
            },
            "required": [
                "image",
                "targetHeight",
                "targetWidth"
            ]
        }
    },
    {
        "name": "post-remove-background-inferences",
        "description": "Advanced remove-background of an image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\"). If provided, image and name will be ignored."
                },
                "backgroundColor": {
                    "type": "string",
                    "description": "The background color as an hexadecimal code (ex: \"#FFFFFF\"), an html color (ex: \"red\") or \"transparent\" if \"format\" is \"png\""
                },
                "format": {
                    "type": "string",
                    "description": "The output format. Default is 'png'",
                    "enum": [
                        "jpeg",
                        "png"
                    ]
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "post-restyle-inferences",
        "description": "Trigger a restyle process from one sketch image (or other image) and one or more reference style images.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The image to restyle. Must reference an existing AssetId or be a data URL."
                },
                "styleFidelity": {
                    "type": "number",
                    "description": "The higher the value the more it will look like the style image(s)"
                },
                "controlEnd": {
                    "type": "number",
                    "description": "End step for control."
                },
                "seed": {
                    "type": "number",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation."
                },
                "styleImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "promptFidelity": {
                    "type": "number",
                    "description": "Increase the fidelity to the prompt during the restyle."
                },
                "clustering": {
                    "type": "boolean",
                    "description": "Activate clustering."
                },
                "sketch": {
                    "type": "boolean",
                    "description": "Activate sketch detection instead of canny."
                },
                "structureFidelity": {
                    "type": "number",
                    "description": "Strength for the input image structure preservation"
                },
                "prompt": {
                    "type": "string",
                    "description": "A full text prompt to guide the restyle process. Default: empty string. Example: \"cute++ chibi character\""
                }
            },
            "required": [
                "image",
                "styleImages"
            ]
        }
    },
    {
        "name": "post-segment-inferences",
        "description": "Trigger the segmentation of an image. The process will create a new Asset with the segmentation mask as a child.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "checkpoint": {
                    "type": "string",
                    "description": "The checkpoint to use",
                    "enum": [
                        "fastsam_x",
                        "sam_b",
                        "sam_h"
                    ]
                },
                "image": {
                    "type": "string",
                    "description": "The input image to process. Must reference an existing AssetId or be a data URL."
                },
                "resultContours": {
                    "type": "boolean",
                    "description": "Boolean to output the contours."
                },
                "dilate": {
                    "type": "number",
                    "description": "The number of pixels to dilate the result masks."
                },
                "bbox": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "backgroundOpacity": {
                    "type": "number",
                    "description": "Int to set between 0 and 255 for the opacity of the background in the result images."
                },
                "betterQuality": {
                    "type": "boolean",
                    "description": "Remove small dark spots (i.e. “pepper”) and connect small bright cracks."
                },
                "text": {
                    "type": "string",
                    "description": "A textual description / keywords describing the object of interest."
                },
                "resultImage": {
                    "type": "boolean",
                    "description": "Boolean to able output the cut out object."
                },
                "points": {
                    "type": "array",
                    "items": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "resultMask": {
                    "type": "boolean",
                    "description": "Boolean to able return the masks (binary image) in the response."
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "post-skybox-base360-inferences",
        "description": "Trigger the generation of a 360 skybox seamless image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The image to use as a starting point for the skybox generation. Must reference an existing AssetId or be a data URL."
                },
                "styleFidelity": {
                    "type": "number",
                    "description": "(deprecated) Condition the influence of the style image. The higher the value, the more the style image will influence the generated skybox image."
                },
                "structureImage": {
                    "type": "string",
                    "description": "The control image for structure. A canny detector will be applied to this image. Must reference an existing AssetId."
                },
                "seed": {
                    "type": "number",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "strength": {
                    "type": "number",
                    "description": "Controls the noise intensity introduced to the input image, where a value of 1.0 completely erases the original image's details. Available for img2img and inpainting. (within [0.01, 1.0], default: 0.75)"
                },
                "styleImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "depthFidelity": {
                    "type": "number",
                    "description": "The depth fidelity if a depth image provided"
                },
                "numOutputs": {
                    "type": "number",
                    "description": "The number of outputs to generate."
                },
                "depthImage": {
                    "type": "string",
                    "description": "The control image processed by depth estimator. Must reference an existing AssetId."
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "A negative full text prompt that discourages the skybox model from generating certain characteristics. It is recommended to test without using a negative prompt. Default: empty string. Example: \"Low resolution, blurry, pixelated, noisy.\""
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation."
                },
                "overrideEmbeddings": {
                    "type": "boolean",
                    "description": "(deprecated) Override the embeddings of the model. Only your prompt and negativePrompt will be used. Use with caution."
                },
                "promptFidelity": {
                    "type": "number",
                    "description": "Increase the fidelity to the prompt during upscale."
                },
                "width": {
                    "type": "number",
                    "description": "The width of the rendered image."
                },
                "style": {
                    "type": "string",
                    "description": "Style to apply for generation.",
                    "enum": [
                        "3d-cartoon",
                        "cartoon",
                        "cinematic",
                        "claymation",
                        "cloud-skydome",
                        "comic",
                        "cyberpunk",
                        "enchanted",
                        "fantasy",
                        "ink",
                        "manga",
                        "manga-color",
                        "neon-tron",
                        "oil-painting",
                        "pastel",
                        "photo",
                        "psychedelic",
                        "retro-fantasy",
                        "scifi-concept-art",
                        "space",
                        "standard",
                        "whimsical"
                    ]
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Controls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "structureFidelity": {
                    "type": "number",
                    "description": "The structure fidelity if a structureImage or a cannyStructureImage image is provided."
                },
                "prompt": {
                    "type": "string",
                    "description": "A full text prompt to guide the skybox generation process. Default: empty string. Example: \"a mountain landscape\""
                },
                "cannyStructureImage": {
                    "type": "string",
                    "description": "The control image already processed by canny detector. Must reference an existing AssetId."
                },
                "geometryEnforcement": {
                    "type": "number",
                    "description": "Apply extra control to the Skybox 360 geometry.\nThe higher the value, the more the 360 geometry will influence the generated skybox image.\n\nUse with caution. Default is adapted to the other parameters."
                }
            },
            "required": [
                "prompt"
            ]
        }
    },
    {
        "name": "post-skybox-upscale360-inferences",
        "description": "Trigger the upscaling of an image matching the 360 skyboxes specific geometry.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "The 360 image to upscale. Must reference an existing AssetId or a data URL."
                },
                "styleFidelity": {
                    "type": "number",
                    "description": "Condition the influence of the style image. The higher the value, the more the style image will influence the upscaled skybox image. Default: 80"
                },
                "sharpen": {
                    "type": "boolean",
                    "description": "Sharpen tiles."
                },
                "seed": {
                    "type": "number",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "styleImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "scalingFactor": {
                    "type": "number",
                    "description": "Scaling factor (when `targetWidth` not specified)"
                },
                "detailsLevel": {
                    "type": "number",
                    "description": "Amount of details to remove or add"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "A negative full text prompt that discourages the skybox upscale from generating certain characteristics. It is recommended to test without using a negative prompt. Default: empty string. Example: \"Low resolution, blurry, pixelated, noisy.\""
                },
                "overrideEmbeddings": {
                    "type": "boolean",
                    "description": "Override the embeddings of the model. Only your prompt and negativePrompt will be used. Use with caution."
                },
                "promptFidelity": {
                    "type": "number",
                    "description": "Increase the fidelity to the prompt during upscale."
                },
                "colorCorrection": {
                    "type": "boolean",
                    "description": "Ensure upscaled tile have the same color histogram as original tile."
                },
                "prompt": {
                    "type": "string",
                    "description": "A full text prompt to guide the skybox upscale. Default: empty string. Example: \"a mountain landscape\""
                },
                "targetWidth": {
                    "type": "number",
                    "description": "Target width for the upscaled image, take priority over scaling factor"
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "post-texture-inferences",
        "description": "Trigger the conversion of an image texture to different texture maps:\n- Height map\n- Normal map\n- Smoothness map\n- Metallic map\n- Edge map\n- Ambient Occlusion map\n\nThe process will create a new Asset with the above texture maps as children + the original image as an Albedo map.\n",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "defaultParameters": {
                    "type": "boolean",
                    "description": "If true, use the default parameters"
                },
                "polished": {
                    "type": "number",
                    "description": "How polished is the surface? 0 is like a rough surface, 1 is like a mirror"
                },
                "angular": {
                    "type": "number",
                    "description": "How angular is the surface? 0 is like a sphere, 1 is like a mechanical object"
                },
                "invert": {
                    "type": "boolean",
                    "description": "To invert the relief"
                },
                "saveFlipbook": {
                    "type": "boolean",
                    "description": "Save a flipbook of the texture. Deactivated when the input texture is larger than 2048x2048px"
                },
                "texture": {
                    "type": "string",
                    "description": "The asset to convert in texture maps. Must reference an existing AssetId."
                },
                "raised": {
                    "type": "number",
                    "description": "How raised is the surface? 0 is flat like water, 1 is like a very rough rock"
                },
                "shiny": {
                    "type": "number",
                    "description": "How shiny is the surface? 0 is like a matte surface, 1 is like a diamond"
                }
            },
            "required": [
                "texture"
            ]
        }
    },
    {
        "name": "post-translate-inferences",
        "description": "Translate text from one language to english.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "dryRun": {
                    "type": "string"
                },
                "ensureIPCleared": {
                    "type": "boolean",
                    "description": "Whether we try to ensure IP removal for new prompt generation."
                },
                "images": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "seed": {
                    "type": "number",
                    "description": "If specified, the API will make a best effort to produce the same results, such that repeated requests with the same `seed` and parameters should return the same outputs. Must be used along with the same parameters including prompt, model's state, etc.."
                },
                "temperature": {
                    "type": "number",
                    "description": "The sampling temperature to use. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic.\n\nWe generally recommend altering this or `topP` but not both."
                },
                "prompt": {
                    "type": "string",
                    "description": "The prompt to translate."
                },
                "topP": {
                    "type": "number",
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So `0.1` means only the tokens comprising the top `10%` probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both."
                }
            },
            "required": [
                "prompt"
            ]
        }
    },
    {
        "name": "post-txt2img-inferences",
        "description": "Trigger a new image generation in Txt2Img mode.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "aspectRatio": {
                    "type": "string",
                    "description": "The aspect ratio of the generated images. Only used for the model flux.1.1-pro-ultra.\nThe aspect ratio is a string formatted as \"width:height\" (example: \"16:9\").",
                    "enum": [
                        "16:9",
                        "1:1",
                        "21:9",
                        "2:3",
                        "3:2",
                        "3:4",
                        "4:3",
                        "4:5",
                        "5:4",
                        "9:16",
                        "9:21"
                    ]
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-txt2img-ip-adapter-inferences",
        "description": "Trigger a new image generation in Txt2Img mode with one IpAdapter reference image that guides the generation process.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "ipAdapterImageIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "ipAdapterImageId": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImageIds` instead.\nThe IpAdapter image as an AssetId. Cannot be set if `ipAdapterImage` is provided. Will be ignored if the `ipAdapterImageIds` parameter is provided."
                },
                "ipAdapterScale": {
                    "type": "number",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterScales` instead.\nIpAdapter scale factor (within [0.0, 1.0], default: 0.9). Will be ignored if the `ipAdapterScales` parameter is provided"
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "ipAdapterType": {
                    "type": "string",
                    "description": "The type of IP Adapter model to use. Must be one of [`style`, `character`], default to `style``",
                    "enum": [
                        "character",
                        "style"
                    ]
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "aspectRatio": {
                    "type": "string",
                    "description": "The aspect ratio of the generated images. Only used for the model flux.1.1-pro-ultra.\nThe aspect ratio is a string formatted as \"width:height\" (example: \"16:9\").",
                    "enum": [
                        "16:9",
                        "1:1",
                        "21:9",
                        "2:3",
                        "3:2",
                        "3:4",
                        "4:3",
                        "4:5",
                        "5:4",
                        "9:16",
                        "9:21"
                    ]
                },
                "ipAdapterScales": {
                    "type": "array",
                    "items": {
                        "type": "number"
                    }
                },
                "ipAdapterImage": {
                    "type": "string",
                    "description": "Deprecated for type txt2img-ip-adapter and img2img-ip-adapter, use `ipAdapterImages` instead.\nThe IpAdapter image as a data url. Will be ignored if the `ipAdapterImages` parameter is provided."
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "ipAdapterImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-txt2img-texture-inferences",
        "description": "Trigger a new seamless texture image generation in Txt2Img mode.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "seed": {
                    "type": "string",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "modelId": {
                    "type": "string",
                    "description": "The model id to use for the inference"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch of the model to use for the inference. Only available for Flux Lora Trained models."
                },
                "hideResults": {
                    "type": "boolean",
                    "description": "If set, generated assets will be hidden and not returned in the list of images of the inference\nor when listing assets (default: false)"
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "The prompt not to guide the image generation, ignored when guidance < 1 (example: \"((ugly face))\")\nFor Flux based model (not Fast-Flux): requires negativePromptStrength > 0 and active only for inference types txt2img / img2img / controlnet."
                },
                "scheduler": {
                    "type": "string",
                    "description": "The scheduler to use to override the default configured for the model. See detailed documentation for more details.",
                    "enum": [
                        "DDIMScheduler",
                        "DDPMScheduler",
                        "DEISMultistepScheduler",
                        "DPMSolverMultistepScheduler",
                        "DPMSolverSinglestepScheduler",
                        "EulerAncestralDiscreteScheduler",
                        "EulerDiscreteScheduler",
                        "HeunDiscreteScheduler",
                        "KDPM2AncestralDiscreteScheduler",
                        "KDPM2DiscreteScheduler",
                        "LCMScheduler",
                        "LMSDiscreteScheduler",
                        "PNDMScheduler",
                        "TCDScheduler",
                        "UniPCMultistepScheduler"
                    ]
                },
                "intermediateImages": {
                    "type": "boolean",
                    "description": "Enable or disable the intermediate images generation (default: false)"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "guidance": {
                    "type": "number",
                    "description": "Controls how closely the generated image follows the prompt. Higher values result in stronger adherence to the prompt. Default and allowed values depend on the model type:\n- For Flux dev models, the default is 3.5 and allowed values are within [0, 10]\n- For Flux pro models, the default is 3 and allowed values are within [2, 5]\n- For SDXL models, the default is 6 and allowed values are within [0, 20]\n- For SD1.5 models, the default is 7.5 and allowed values are within [0, 20]"
                },
                "numInferenceSteps": {
                    "type": "number",
                    "description": "The number of denoising steps for each image generation (within [1, 150], default: 30)"
                },
                "numSamples": {
                    "type": "number",
                    "description": "The number of images to generate (within [1, 128], default: 4)"
                },
                "width": {
                    "type": "number",
                    "description": "The width of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the width must be within [512, 2048]\nIf model.type is `sd-1_5`, the width must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                },
                "negativePromptStrength": {
                    "type": "number",
                    "description": "Only applicable for flux-dev based models for `txt2img`, `img2img`, and `controlnet` inference types.\n\nControls the influence of the negative prompt. Default 0 means the negative prompt has no effect. Higher values increase negative prompt influence.\nMust be > 0 if negativePrompt is provided."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The base model to use for the inference. Only Flux LoRA models can use this parameter.\nAllowed values are available in the model's attribute: `compliantModelIds`"
                },
                "prompt": {
                    "type": "string",
                    "description": "Full text prompt including the model placeholder. (example: \"an illustration of phoenix in a fantasy world, flying over a mountain, 8k, bokeh effect\")"
                },
                "height": {
                    "type": "number",
                    "description": "The height of the generated images, must be a 8 multiple (within [64, 2048], default: 512)\nIf model.type is `sd-xl`, `sd-xl-lora`, `sd-xl-composition` the height must be within [512, 2048]\nIf model.type is `sd-1_5`, the height must be within [64, 1024]\nIf model.type is `flux.1.1-pro-ultra`, you can use the aspectRatio parameter instead"
                }
            },
            "required": [
                "modelId",
                "prompt"
            ]
        }
    },
    {
        "name": "post-upscale-inferences",
        "description": "Trigger the upscaling of an image. You can use styles and presets to quickly get results or craft your very own settings.\n\n**Note**:This endpoint is deprecated and will be removed in the future. Please leverage `POST /generate/custom/{modelId}` endpoint instead with `model_scenario-upscale-v3` `modelId` for example. See https://docs.scenario.com/docs/upscale-generation for more details.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "image": {
                    "type": "string",
                    "description": "Image to upscale. Must reference an existing AssetId or be a data URL."
                },
                "creativityDecay": {
                    "type": "number",
                    "description": "Amount of decay in creativity over the upscale process. The lowest the value, the less the creativity will be preserved over the upscale process."
                },
                "tileStyle": {
                    "type": "boolean",
                    "description": "If set to true, during the upscaling process, the model will match tiles of the source image with tiles of the style image(s). This will result in a more coherent restyle. Works best with style images that have a similar composition."
                },
                "seed": {
                    "type": "number",
                    "description": "Used to reproduce previous results. Default: randomly generated number."
                },
                "styleImages": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "scalingFactor": {
                    "type": "number",
                    "description": "Scaling factor (when `targetWidth` not specified)"
                },
                "preset": {
                    "type": "string",
                    "description": "Optimize the upscale for a specific use case. Precise: Upscale for high fidelity. Balanced: Upscale for a balance between fidelity and creativity. Creative: Upscale for creativity.",
                    "enum": [
                        "balanced",
                        "creative",
                        "precise"
                    ]
                },
                "styleImagesFidelity": {
                    "type": "number",
                    "description": "Condition the influence of the style image(s). The higher the value, the more the style images will influence the upscaled image."
                },
                "detailsLevel": {
                    "type": "number",
                    "description": "Amount of details to remove or add"
                },
                "fractality": {
                    "type": "number",
                    "description": "Determine the scale at which the upscale process works.\n- With a small value, the upscale works at the largest scale, resulting in fewer added details and more coherent images. Ideal for portraits, for example.\n- With a large value, the upscale works at the smallest scale, resulting in more added details and more hallucinations. Ideal for landscapes, for example.\n\n(info): A small value is slower and more expensive to run."
                },
                "negativePrompt": {
                    "type": "string",
                    "description": "A negative full text prompt that discourages the upscale from generating certain characteristics. It is recommended to test without using a negative prompt. Default: empty string. Example: \"Low resolution, blurry, pixelated, noisy.\""
                },
                "overrideEmbeddings": {
                    "type": "boolean",
                    "description": "Override the embeddings of the model. Only your prompt and negativePrompt will be used. Use with caution."
                },
                "promptFidelity": {
                    "type": "number",
                    "description": "Increase the fidelity to the prompt during upscale. Default: optimized for your preset and style."
                },
                "style": {
                    "type": "string",
                    "description": "Optimize the upscale for a specific style. standard works in most cases. Use one of the other styles to refine the outputs.",
                    "enum": [
                        "3d-rendered",
                        "anime",
                        "cartoon",
                        "comic",
                        "minimalist",
                        "photography",
                        "standard"
                    ]
                },
                "refinementSteps": {
                    "type": "number",
                    "description": "Additional refinement steps before scaling.\n\nIf scalingFactor == 1, the refinement process will be applied (1 + refinementSteps) times.\nIf scalingFactor > 1, the refinement process will be applied refinementSteps times."
                },
                "creativity": {
                    "type": "number",
                    "description": "Allow the generation of \"hallucinations\" during the upscale process, which adds additional details and deviates from the original image. Default: optimized for your preset and style."
                },
                "imageFidelity": {
                    "type": "number",
                    "description": "Strengthen the similarity to the original image during the upscale. Default: optimized for your preset and style."
                },
                "imageType": {
                    "type": "string",
                    "description": "Preserve the seamless properties of skybox or texture images. Input has to be of same type (seamless).",
                    "enum": [
                        "seamfull",
                        "skybox",
                        "texture"
                    ]
                },
                "prompt": {
                    "type": "string",
                    "description": "A full text prompt to guide the upscale and forcing the generation of certain characteristics. Default: empty string. Example: \"UHD 8K hyper detailed studio photo of man face with yellow skin, anatomical++, disturbing+++, black background. Bloody++.\""
                },
                "targetWidth": {
                    "type": "number",
                    "description": "Target width for the upscaled image, take priority over scaling factor"
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "post-vectorize-inferences",
        "description": "Advanced vectorization of an image.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "mode": {
                    "type": "string",
                    "description": "Curver fitting mode `none`, `polygon`, `spline`",
                    "enum": [
                        "none",
                        "polygon",
                        "spline"
                    ]
                },
                "image": {
                    "type": "string",
                    "description": "The asset ID (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\") to vectorize."
                },
                "layerDifference": {
                    "type": "number",
                    "description": "Represents the color difference between gradient layers (higher value will reduce the number of layers)\n\nOnly applicable to `color` colorMode."
                },
                "maxIterations": {
                    "type": "number",
                    "description": "Max iterations for rendering"
                },
                "cornerThreshold": {
                    "type": "number",
                    "description": "Minimum momentary angle (degree) to be considered a corner (higher value will smooth corners)\n\nOnly applicable to `spline` mode."
                },
                "colorPrecision": {
                    "type": "number",
                    "description": "Number of significant bits to use in an RGB channel, min 1, max 16 (higher value will increase precision)\n\nOnly applicable to `color` colorMode."
                },
                "spliceThreshold": {
                    "type": "number",
                    "description": "Minimum angle displacement (degree) to splice a spline (higher value reduce accuracy)\n\nOnly applicable to `spline` mode."
                },
                "lengthThreshold": {
                    "type": "number",
                    "description": "Minimum length of a segment (higher value will generate more coarse output)\n\nOnly applicable to `spline` mode."
                },
                "colorMode": {
                    "type": "string",
                    "description": "Color mode `bw`, `color`. If `bw`, the image will be considered as black and white.",
                    "enum": [
                        "bw",
                        "color"
                    ]
                },
                "filterSpeckle": {
                    "type": "number",
                    "description": "Discard patches smaller than X px in size (higher value will reduce the number of patches, cleaner output)"
                },
                "preset": {
                    "type": "string",
                    "description": "If preset given, all other parameters will be ignored (mode, colorMode, filterSpeckle, ...), except for custom.",
                    "enum": [
                        "asset",
                        "bw",
                        "custom",
                        "photo",
                        "pixelart",
                        "poster"
                    ]
                },
                "pathPrecision": {
                    "type": "number",
                    "description": "Number of decimal places to use in path string"
                }
            },
            "required": [
                "image"
            ]
        }
    },
    {
        "name": "get-jobs",
        "description": "List all jobs matching the given filters. A job is a synchronous operation or an asynchronous task such as a training, a generation, etc. It offers a unified view of all operations running on the platform along with their status and results.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "authorId": {
                    "type": "string",
                    "description": "The authorId of the jobs to return. Optional."
                },
                "hideResults": {
                    "type": "string",
                    "description": "If false, jobs containing the hideResults param will be not returned. Optional."
                },
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 10, maximum value is 200, minimum value is 1"
                },
                "type": {
                    "type": "string",
                    "description": "The type of the jobs to return. If \"types\" is defined, \"type\" will be ignored. Optional."
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                },
                "types": {
                    "type": "string",
                    "description": "The types of the jobs to return. If \"types\" is defined, \"type\" will be ignored. Optional."
                },
                "status": {
                    "type": "string",
                    "description": "The status of the jobs to return. Optional."
                }
            },
            "required": []
        }
    },
    {
        "name": "get-job-id",
        "description": "Get job data by job ID",
        "inputSchema": {
            "type": "object",
            "properties": {
                "jobId": {
                    "type": "string"
                }
            },
            "required": [
                "jobId"
            ]
        }
    },
    {
        "name": "post-job-action-by-job-id",
        "description": "Trigger an action on a job: cancel",
        "inputSchema": {
            "type": "object",
            "properties": {
                "jobId": {
                    "type": "string",
                    "description": "The job Id"
                },
                "action": {
                    "type": "string",
                    "description": "The action to execute on the job, such as canceling it. Today only cancel on inference jobs is supported.",
                    "enum": [
                        "cancel"
                    ]
                }
            },
            "required": [
                "jobId"
            ]
        }
    },
    {
        "name": "get-models",
        "description": "List all models",
        "inputSchema": {
            "type": "object",
            "properties": {
                "privacy": {
                    "type": "string",
                    "description": "The privacy of the models to return. The default value is `private`, possible values are `private` and `public`"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 100, maximum value is 500, minimum value is 1"
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                },
                "blacklisted": {
                    "type": "string",
                    "description": "If set to true, returns the list of models blacklisted for the team (only available for team admins)"
                },
                "status": {
                    "type": "string",
                    "description": "The status of the models to return"
                },
                "collectionId": {
                    "type": "string",
                    "description": "When provided, only the models in the Collection will be returned"
                },
                "loadedOnly": {
                    "type": "string",
                    "description": "If set to true, returns the list of models currently loaded on GPU"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-models",
        "description": "Create a new model",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "collectionIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "classSlug": {
                    "type": "string",
                    "description": "The slug of the class you want to use (ex: \"characters-npcs-mobs-characters\"). Set to null to unset the class"
                },
                "name": {
                    "type": "string",
                    "description": "The model's name (ex: \"Cinematic Realism\").\n\nIf not set, the model's name will be automatically generated when starting training based on training data."
                },
                "shortDescription": {
                    "type": "string",
                    "description": "The model's short description (ex: \"This model generates highly detailed cinematic scenes.\").\n\nIf not set, the model's short description will be automatically generated when starting training based on training data."
                },
                "baseModelId": {
                    "type": "string",
                    "description": "The ID of the base model to use as a starting point for the training (example: \"flux.1-dev\")\n\nValue is automatically set based on the model's type. In case of doubt leave it empty."
                },
                "type": {
                    "type": "string",
                    "description": "The model's type (ex: \"flux.1-lora\").\n\nThe type can only be changed if the model has the \"new\" status.",
                    "enum": [
                        "custom",
                        "flux.1",
                        "flux.1-composition",
                        "flux.1-kontext-dev",
                        "flux.1-kontext-lora",
                        "flux.1-krea-dev",
                        "flux.1-krea-lora",
                        "flux.1-lora",
                        "flux.1-pro",
                        "flux.1.1-pro-ultra",
                        "flux1.1-pro",
                        "gpt-image-1",
                        "sd-1_5",
                        "sd-1_5-composition",
                        "sd-1_5-lora",
                        "sd-xl",
                        "sd-xl-composition",
                        "sd-xl-lora"
                    ]
                }
            },
            "required": []
        }
    },
    {
        "name": "post-models-get-bulk",
        "description": "Get multiple models by their `modelIds`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "settings": {
                    "type": "boolean",
                    "description": "If true, will return the settings: `promptEmbedding` and `negativePromptEmbedding`."
                },
                "trainingImagesPreview": {
                    "type": "boolean",
                    "description": "If true will return the first 3 training images; otherwise returns the full training images.\n\nIf `allTrainingImages` set to true, this parameter is ignored."
                },
                "minimal": {
                    "type": "boolean",
                    "description": "If true will return only the base details of the model (id, name, type)\n\nif true, all other parameters are ignored"
                },
                "thumbnail": {
                    "type": "boolean",
                    "description": "If true will return the thumbnail, when no thumbnail is set, will try to fetch the first training image instead."
                },
                "allTrainingImages": {
                    "type": "boolean",
                    "description": "If true will return all training images; otherwise returns only the first 3 training images.\n\nIf `trainingImagesPreview` set to true, this parameter is ignored."
                },
                "modelIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": []
        }
    },
    {
        "name": "get-public-models",
        "description": "List all public models",
        "inputSchema": {
            "type": "object",
            "properties": {
                "updatedBefore": {
                    "type": "string",
                    "description": "Filter results to only return models updated before the specified ISO string date (exclusive). Requires the sortBy parameter to be \"updatedAt\""
                },
                "sortDirection": {
                    "type": "string",
                    "description": "Sort results in ascending (asc) or descending (desc) order"
                },
                "collectionIds": {
                    "type": "string",
                    "description": "List of collection ids, comma separated."
                },
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 50, maximum value is 500, minimum value is 1"
                },
                "type": {
                    "type": "string",
                    "description": "List all the models of a specific type. Can be any of the following values: sd-1_5, sd-1_5-lora, sd-1_5-composition, sd-xl, sd-xl-lora, sd-xl-composition, flux.1, flux.1-lora, flux.1-kontext-dev, flux.1-krea-dev, flux.1-kontext-lora, flux.1-krea-lora, flux.1-composition, flux.1-pro, flux1.1-pro, flux.1.1-pro-ultra, gpt-image-1, custom"
                },
                "updatedAfter": {
                    "type": "string",
                    "description": "Filter results to only return models updated after the specified ISO string date (exclusive). Requires the sortBy parameter to be \"updatedAt\""
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                },
                "tags": {
                    "type": "string",
                    "description": "List of tags, comma separated."
                },
                "types": {
                    "type": "string",
                    "description": "List of types, comma separated. Can be any of the following values: sd-1_5, sd-1_5-lora, sd-1_5-composition, sd-xl, sd-xl-lora, sd-xl-composition, flux.1, flux.1-lora, flux.1-kontext-dev, flux.1-krea-dev, flux.1-kontext-lora, flux.1-krea-lora, flux.1-composition, flux.1-pro, flux1.1-pro, flux.1.1-pro-ultra, gpt-image-1, custom"
                },
                "createdBefore": {
                    "type": "string",
                    "description": "Filter results to only return models created before the specified ISO string date (exclusive). Requires the sortBy parameter to be \"createdAt\""
                },
                "sortBy": {
                    "type": "string",
                    "description": "Sort results by the createdAt or updatedAt"
                },
                "createdAfter": {
                    "type": "string",
                    "description": "Filter results to only return models created after the specified ISO string date (exclusive). Requires the sortBy parameter to be \"createdAt\""
                }
            },
            "required": []
        }
    },
    {
        "name": "get-public-models-by-model-id",
        "description": "Get the details of the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The model ID to retrieve"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "get-models-by-model-id",
        "description": "Get the details of the given `modelId`, including its training status and training progress if available",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model's `modelId` to retrieve"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "put-models-by-model-id",
        "description": "Update the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The model's `modelId` to update"
                },
                "negativePromptEmbedding": {
                    "type": "string",
                    "description": "Add a negative prompt embedding to every model's generation"
                },
                "thumbnail": {
                    "type": "string",
                    "description": "The AssetId of the image you want to use as a thumbnail for the model (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\"). Set to null to unset the thumbnail"
                },
                "concepts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "modelId": {
                                "type": "string",
                                "description": "The model ID (example: \"model_eyVcnFJcR92BxBkz7N6g5w\")"
                            },
                            "modelEpoch": {
                                "type": "string",
                                "description": "The epoch of the model (example: \"000001\")\nOnly available for Flux Lora Trained models"
                            },
                            "scale": {
                                "type": "number",
                                "description": "The scale of the model (example: 1.0)\nFor Flux Kontext Prompt Editing, the scale is between 0 and 2."
                            }
                        },
                        "required": [
                            "modelId",
                            "scale"
                        ]
                    }
                },
                "classSlug": {
                    "type": "string",
                    "description": "The slug of the class you want to use (ex: \"characters-npcs-mobs-characters\"). Set to null to unset the class"
                },
                "name": {
                    "type": "string",
                    "description": "The model's name (ex: \"Cinematic Realism\").\n\nIf not set, the model's name will be automatically generated when starting training based on training data."
                },
                "epoch": {
                    "type": "string",
                    "description": "The epoch of the model. Only available for flux.1-lora and flux.1-kontext-lora based models.\n\nThe epoch can only be set if the model has epochs and is in status \"trained\".\n\nThe default epoch (if not set) is the final model epoch (latest).\n\nSet to null to unset the epoch."
                },
                "promptEmbedding": {
                    "type": "string",
                    "description": "Add a prompt embedding to every model's generation"
                },
                "shortDescription": {
                    "type": "string",
                    "description": "The model's short description (ex: \"This model generates highly detailed cinematic scenes.\").\n\nIf not set, the model's short description will be automatically generated when starting training based on training data."
                },
                "type": {
                    "type": "string",
                    "description": "The model's type (ex: \"flux.1-lora\").\n\nThe type can only be changed if the model has the \"new\" status.",
                    "enum": [
                        "custom",
                        "flux.1",
                        "flux.1-composition",
                        "flux.1-kontext-dev",
                        "flux.1-kontext-lora",
                        "flux.1-krea-dev",
                        "flux.1-krea-lora",
                        "flux.1-lora",
                        "flux.1-pro",
                        "flux.1.1-pro-ultra",
                        "flux1.1-pro",
                        "gpt-image-1",
                        "sd-1_5",
                        "sd-1_5-composition",
                        "sd-1_5-lora",
                        "sd-xl",
                        "sd-xl-composition",
                        "sd-xl-lora"
                    ]
                },
                "parameters": {
                    "type": "object",
                    "properties": {
                        "priorLossWeight": {
                            "type": "number",
                            "description": "The weight of prior preservation loss\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "seed": {
                            "type": "number",
                            "description": "Used to reproduce previous results. Default: randomly generated number.\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "numUNetTrainSteps": {
                            "type": "number",
                            "description": "The number of training steps for the UNet\n\nOnly available for SDXL LoRA training"
                        },
                        "sampleSourceImages": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "classPrompt": {
                            "type": "string",
                            "description": "The prompt to specify images in the same class as provided instance images\n\nOnly available for SD15 training"
                        },
                        "wandbKey": {
                            "type": "string",
                            "description": "The Weights And Bias key to use for logging. The maximum length is 40 characters"
                        },
                        "scaleLr": {
                            "type": "boolean",
                            "description": "Whether to scale the learning rate\n\nNote: Legacy parameter, will be ignored\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "randomCropScale": {
                            "type": "number",
                            "description": "Scale of random crops\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "lrScheduler": {
                            "type": "string",
                            "description": "The scheduler type to use (default: \"constant\")\n\nOnly available for SD15 and SDXL LoRA training",
                            "enum": [
                                "constant",
                                "constant-with-warmup",
                                "cosine",
                                "cosine-with-restarts",
                                "linear",
                                "polynomial"
                            ]
                        },
                        "rank": {
                            "type": "number",
                            "description": "The dimension of the LoRA update matrices\n\nOnly available for SDXL and Flux LoRA training\n\nDefault value varies depending on the model type:\n- For SDXL: 64\n- For Flux: 16"
                        },
                        "validationPrompt": {
                            "type": "string",
                            "description": "Validation prompt\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "conceptPrompt": {
                            "type": "string",
                            "description": "The prompt with identifier specifying the instance (or subject) of the class (example: \"a daiton dog\")\n\nDefault value varies depending on the model type:\n- For SD1.5: \"daiton\" if no class is associated with the model\n- For SDXL: \"daiton\"\n- For Flux: \"\""
                        },
                        "maxTrainSteps": {
                            "type": "number",
                            "description": "Maximum number of training steps to execute (default: varies depending on the model type)\n\nFor SDXL LoRA training, please use `numTextTrainSteps` and `numUNetTrainSteps` instead\n\nDefault value varies depending on the model type:\n- For SD1.5: round((number of training images * 225) / 3)\n- For SDXL: number of training images * 175\n- For Flux: number of training images * 100\n\nMaximum value varies depending on the model type:\n- For SD1.5 and SDXL: [0, 40000]\n- For Flux: [0, 10000]"
                        },
                        "nbEpochs": {
                            "type": "number",
                            "description": "The number of epochs to train for\n\nOnly available for Flux LoRA training"
                        },
                        "textEncoderTrainingRatio": {
                            "type": "number",
                            "description": "Whether to train the text encoder or not\n\nExample: For 100 steps and a value of 0.2, it means that the text encoder will be trained for 20 steps and then the UNet for 80 steps\n\nNote: Legacy parameter, please use `numTextTrainSteps` and `numUNetTrainSteps`\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "validationFrequency": {
                            "type": "number",
                            "description": "Validation frequency. Cannot be greater than maxTrainSteps value\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "nbRepeats": {
                            "type": "number",
                            "description": "The number of times to repeat the training\n\nOnly available for Flux LoRA training"
                        },
                        "samplePrompts": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "randomCropRatio": {
                            "type": "number",
                            "description": "Ratio of random crops\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "learningRate": {
                            "type": "number",
                            "description": "Initial learning rate (after the potential warmup period)\n\nDefault value varies depending on the model type:\n- For SD1.5 and SDXL: 0.000005\n- For Flux: 0.0001"
                        },
                        "optimizeFor": {
                            "type": "string",
                            "description": "Optimize the model training task for a specific type of input images. The available values are:\n- \"likeness\": optimize training for likeness or portrait (targets specific transformer blocks)\n- \"all\": train all transformer blocks\n- \"none\": train no specific transformer blocks\n\nThis parameter controls which double and single transformer blocks are trained\nduring the LoRA training process.\n\nOnly available for Flux LoRA training",
                            "enum": [
                                "likeness"
                            ]
                        },
                        "randomCrop": {
                            "type": "boolean",
                            "description": "Whether to random crop or center crop images before resizing to the working resolution\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "numTextTrainSteps": {
                            "type": "number",
                            "description": "The number of training steps for the text encoder\n\nOnly available for SDXL LoRA training"
                        },
                        "learningRateTextEncoder": {
                            "type": "number",
                            "description": "Initial learning rate (after the potential warmup period) for the text encoder\n\nMaximum [Flux LoRA: 0.001]\nDefault [SDXL: 0.00005 | Flux LoRA: 0.00001]\nMinimum [SDXL: 0 | Flux LoRA: 0.000001]"
                        },
                        "learningRateUnet": {
                            "type": "number",
                            "description": "Initial learning rate (after the potential warmup period) for the UNet\n\nOnly available for SDXL LoRA training"
                        },
                        "batchSize": {
                            "type": "number",
                            "description": "The batch size\nLess steps, and will increase the learning rate\n\nOnly available for Flux LoRA training"
                        }
                    },
                    "required": []
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "delete-models-by-model-id",
        "description": "Delete a model",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The modelId to delete"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "post-models-copy-by-model-id",
        "description": "Copy the given `modelId` to a new model, thumbnail, presets, and all of its training images and pairs if any",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The modelId to copy"
                },
                "copyExamples": {
                    "type": "boolean",
                    "description": "true by default, the example images will be copied"
                },
                "copyAsTrained": {
                    "type": "boolean",
                    "description": "If set to true, the training data will be copied"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "get-models-description-by-model-id",
        "description": "Get the description of the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The description's `modelId` to retrieve"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "put-models-description-by-model-id",
        "description": "Update the markdown description of the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The description's `modelId` to update"
                },
                "description": {
                    "type": "string",
                    "description": "The markdown description of the model (ex: `# My model`). Set to `null` to delete the description."
                }
            },
            "required": [
                "modelId",
                "description"
            ]
        }
    },
    {
        "name": "post-download-model",
        "description": "Request a link to download the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string"
                },
                "modelEpoch": {
                    "type": "string",
                    "description": "The epoch hash of the model to download\nOnly available for Flux Lora Trained models with epochs\nWill only apply to the main model in the download request\nIf not set, the default (latest or setup at model level) epoch will be used"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "get-models-examples-by-model-id",
        "description": "List all examples of the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The examples' `modelId` to retrieve"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "put-models-examples-by-model-id",
        "description": "Add/delete/sort examples of the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The examples' `modelId` to update"
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "modelId",
                "assetIds"
            ]
        }
    },
    {
        "name": "delete-models-images-by-model-id",
        "description": "Delete an image",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The images' `modelId` to delete"
                },
                "ids": {
                    "type": "string",
                    "description": "The asset ids of the images to delete"
                }
            },
            "required": [
                "modelId",
                "ids"
            ]
        }
    },
    {
        "name": "get-model-presets-by-model-id",
        "description": "List all presets for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The presets' `modelId`"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "post-model-preset-by-model-id",
        "description": "Create a new preset for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The preset's `modelId`"
                },
                "isDefault": {
                    "type": "boolean",
                    "description": "Whether this preset should be the default preset for the model"
                },
                "inferenceId": {
                    "type": "string",
                    "description": "The inference ID used to generate new images"
                }
            },
            "required": [
                "modelId",
                "inferenceId"
            ]
        }
    },
    {
        "name": "put-model-preset-by-model-id-and-preset-id",
        "description": "Modify the given `presetId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The preset's `modelId`"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "presetId": {
                    "type": "string"
                },
                "isDefault": {
                    "type": "boolean",
                    "description": "Whether this preset should be the default preset for the model"
                }
            },
            "required": [
                "modelId",
                "presetId",
                "isDefault"
            ]
        }
    },
    {
        "name": "delete-model-preset-by-model-id-and-preset-id",
        "description": "Delete a preset for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The preset's `modelId` to delete"
                },
                "presetId": {
                    "type": "string",
                    "description": "The preset's `presetId` to delete"
                }
            },
            "required": [
                "modelId",
                "presetId"
            ]
        }
    },
    {
        "name": "get-models-scores-prompt-by-model-id",
        "description": "Get the prompt scores for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The prompt scores' `modelId`"
                },
                "prompt": {
                    "type": "string"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "get-models-scores-training-dataset-by-model-id",
        "description": "Get the training dataset scores for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The training dataset scores' `modelId`"
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "put-models-tags-by-model-id",
        "description": "Add/delete tags for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The tags' `modelId`"
                },
                "add": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "strict": {
                    "type": "boolean",
                    "description": "If true, the function will throw an error if:\n- one of the tags to add already exists\n- one of the tags to delete is not found\nIf false, the endpoint will behave as if it was idempotent"
                },
                "delete": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "put-models-train-by-model-id",
        "description": "Trigger the given `modelId` training",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The training's `modelId` to trigger"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "dryRun": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "properties": {
                        "priorLossWeight": {
                            "type": "number",
                            "description": "The weight of prior preservation loss\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "seed": {
                            "type": "number",
                            "description": "Used to reproduce previous results. Default: randomly generated number.\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "numUNetTrainSteps": {
                            "type": "number",
                            "description": "The number of training steps for the UNet\n\nOnly available for SDXL LoRA training"
                        },
                        "sampleSourceImages": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "classPrompt": {
                            "type": "string",
                            "description": "The prompt to specify images in the same class as provided instance images\n\nOnly available for SD15 training"
                        },
                        "wandbKey": {
                            "type": "string",
                            "description": "The Weights And Bias key to use for logging. The maximum length is 40 characters"
                        },
                        "scaleLr": {
                            "type": "boolean",
                            "description": "Whether to scale the learning rate\n\nNote: Legacy parameter, will be ignored\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "randomCropScale": {
                            "type": "number",
                            "description": "Scale of random crops\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "lrScheduler": {
                            "type": "string",
                            "description": "The scheduler type to use (default: \"constant\")\n\nOnly available for SD15 and SDXL LoRA training",
                            "enum": [
                                "constant",
                                "constant-with-warmup",
                                "cosine",
                                "cosine-with-restarts",
                                "linear",
                                "polynomial"
                            ]
                        },
                        "rank": {
                            "type": "number",
                            "description": "The dimension of the LoRA update matrices\n\nOnly available for SDXL and Flux LoRA training\n\nDefault value varies depending on the model type:\n- For SDXL: 64\n- For Flux: 16"
                        },
                        "validationPrompt": {
                            "type": "string",
                            "description": "Validation prompt\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "conceptPrompt": {
                            "type": "string",
                            "description": "The prompt with identifier specifying the instance (or subject) of the class (example: \"a daiton dog\")\n\nDefault value varies depending on the model type:\n- For SD1.5: \"daiton\" if no class is associated with the model\n- For SDXL: \"daiton\"\n- For Flux: \"\""
                        },
                        "maxTrainSteps": {
                            "type": "number",
                            "description": "Maximum number of training steps to execute (default: varies depending on the model type)\n\nFor SDXL LoRA training, please use `numTextTrainSteps` and `numUNetTrainSteps` instead\n\nDefault value varies depending on the model type:\n- For SD1.5: round((number of training images * 225) / 3)\n- For SDXL: number of training images * 175\n- For Flux: number of training images * 100\n\nMaximum value varies depending on the model type:\n- For SD1.5 and SDXL: [0, 40000]\n- For Flux: [0, 10000]"
                        },
                        "nbEpochs": {
                            "type": "number",
                            "description": "The number of epochs to train for\n\nOnly available for Flux LoRA training"
                        },
                        "textEncoderTrainingRatio": {
                            "type": "number",
                            "description": "Whether to train the text encoder or not\n\nExample: For 100 steps and a value of 0.2, it means that the text encoder will be trained for 20 steps and then the UNet for 80 steps\n\nNote: Legacy parameter, please use `numTextTrainSteps` and `numUNetTrainSteps`\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "validationFrequency": {
                            "type": "number",
                            "description": "Validation frequency. Cannot be greater than maxTrainSteps value\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "nbRepeats": {
                            "type": "number",
                            "description": "The number of times to repeat the training\n\nOnly available for Flux LoRA training"
                        },
                        "samplePrompts": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "randomCropRatio": {
                            "type": "number",
                            "description": "Ratio of random crops\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "learningRate": {
                            "type": "number",
                            "description": "Initial learning rate (after the potential warmup period)\n\nDefault value varies depending on the model type:\n- For SD1.5 and SDXL: 0.000005\n- For Flux: 0.0001"
                        },
                        "optimizeFor": {
                            "type": "string",
                            "description": "Optimize the model training task for a specific type of input images. The available values are:\n- \"likeness\": optimize training for likeness or portrait (targets specific transformer blocks)\n- \"all\": train all transformer blocks\n- \"none\": train no specific transformer blocks\n\nThis parameter controls which double and single transformer blocks are trained\nduring the LoRA training process.\n\nOnly available for Flux LoRA training",
                            "enum": [
                                "likeness"
                            ]
                        },
                        "randomCrop": {
                            "type": "boolean",
                            "description": "Whether to random crop or center crop images before resizing to the working resolution\n\nOnly available for SD15 and SDXL LoRA training"
                        },
                        "numTextTrainSteps": {
                            "type": "number",
                            "description": "The number of training steps for the text encoder\n\nOnly available for SDXL LoRA training"
                        },
                        "learningRateTextEncoder": {
                            "type": "number",
                            "description": "Initial learning rate (after the potential warmup period) for the text encoder\n\nMaximum [Flux LoRA: 0.001]\nDefault [SDXL: 0.00005 | Flux LoRA: 0.00001]\nMinimum [SDXL: 0 | Flux LoRA: 0.000001]"
                        },
                        "learningRateUnet": {
                            "type": "number",
                            "description": "Initial learning rate (after the potential warmup period) for the UNet\n\nOnly available for SDXL LoRA training"
                        },
                        "batchSize": {
                            "type": "number",
                            "description": "The batch size\nLess steps, and will increase the learning rate\n\nOnly available for Flux LoRA training"
                        }
                    },
                    "required": []
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "post-model-training-action-by-model-id",
        "description": "Trigger an action on a model training: cancel",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The `modelId` being trained"
                },
                "action": {
                    "type": "string",
                    "description": "The action to perform on the model training",
                    "enum": [
                        "cancel"
                    ]
                }
            },
            "required": [
                "modelId",
                "action"
            ]
        }
    },
    {
        "name": "post-models-training-images-by-model-id",
        "description": "Add a new training image to the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "modelId": {
                    "type": "string",
                    "description": "The `modelId` where the training image will be stored"
                },
                "data": {
                    "type": "string",
                    "description": "The training image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "assetId": {
                    "type": "string",
                    "description": "The asset ID to use as a training image (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\"). If provided, \"data\" and \"name\" parameters will be ignored."
                },
                "name": {
                    "type": "string",
                    "description": "The original file name of the image (example: \"my-training-image.jpg\")"
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "preset": {
                    "type": "string",
                    "description": "The preset to use for training images",
                    "enum": [
                        "default",
                        "style",
                        "subject"
                    ]
                }
            },
            "required": [
                "modelId"
            ]
        }
    },
    {
        "name": "put-models-training-images-pairs-by-model-id",
        "description": "Replace all training image pairs for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The `modelId` where the training image pairs will be stored"
                },
                "body": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "sourceId": {
                                "type": "string",
                                "description": "The source asset ID (must be a training asset)"
                            },
                            "targetId": {
                                "type": "string",
                                "description": "The target asset ID (must be a training asset)"
                            },
                            "instruction": {
                                "type": "string",
                                "description": "The instruction for the image pair, source to target"
                            }
                        },
                        "required": []
                    }
                }
            },
            "required": [
                "modelId",
                "body"
            ]
        }
    },
    {
        "name": "put-models-training-images-by-model-id-and-training-image-id",
        "description": "Replace the given `trainingImageId` for the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The training image's `modelId`"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "trainingImageId": {
                    "type": "string",
                    "description": "The training image's `trainingImageId` to replace"
                },
                "data": {
                    "type": "string",
                    "description": "The training image as a data URL (example: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVQYV2NgYAAAAAMAAWgmWQ0AAAAASUVORK5CYII=\")"
                },
                "assetId": {
                    "type": "string",
                    "description": "The asset ID to use as a training image (example: \"asset_GTrL3mq4SXWyMxkOHRxlpw\"). If provided, \"data\" and \"name\" parameters will be ignored."
                },
                "name": {
                    "type": "string",
                    "description": "The original file name of the image (example: \"my-training-image.jpg\")"
                },
                "assetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "preset": {
                    "type": "string",
                    "description": "The preset to use for training images",
                    "enum": [
                        "default",
                        "style",
                        "subject"
                    ]
                }
            },
            "required": [
                "modelId",
                "trainingImageId"
            ]
        }
    },
    {
        "name": "delete-models-training-images-by-model-id-and-training-image-id",
        "description": "Delete the given `trainingImageId` from the given `modelId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string",
                    "description": "The training image's `modelId`"
                },
                "trainingImageId": {
                    "type": "string",
                    "description": "The training image's `trainingImageId` to delete"
                }
            },
            "required": [
                "modelId",
                "trainingImageId"
            ]
        }
    },
    {
        "name": "post-models-transfer-by-model-id",
        "description": "Transfer (with a copy or a full ownership change) a model to a new owner, including all of its training images",
        "inputSchema": {
            "type": "object",
            "properties": {
                "modelId": {
                    "type": "string"
                },
                "destinationProjectId": {
                    "type": "string",
                    "description": "The id of the project to copy and transfer the model to"
                },
                "destinationTeamId": {
                    "type": "string",
                    "description": "The id of the team to copy and transfer the model to"
                }
            },
            "required": [
                "modelId",
                "destinationProjectId"
            ]
        }
    },
    {
        "name": "get-public-oscu-prices",
        "description": "Get the public Prepaid Compute Units (or OSCU for One Shot Compute Units) price details",
        "inputSchema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "get-recommendations-models",
        "description": "List recommended models matching the given filters",
        "inputSchema": {
            "type": "object",
            "properties": {
                "capabilities": {
                    "type": "string",
                    "description": "Filter models by capabilities. Multiple values comma-separated.\nExamples: `txt2img`, `img2img`, `inpaint`, `controlnet`, `txt2img,img2img,2img`. Also accepts prefixes or suffixes values such as `txt2`, `img2`, `inpaint`, `control`. Default: no filter"
                },
                "excludeModelIds": {
                    "type": "string",
                    "description": "Exclude specific models by their IDs. Multiple IDs comma-separated. Example: `model1,model2,model3`. Default: no exclusions"
                },
                "limit": {
                    "type": "string",
                    "description": "The maximum number of models to return. Default: `10`, Maximum: `30`"
                },
                "nextToken": {
                    "type": "string",
                    "description": "Pagination token to retrieve the next page of results. Use the `nextToken` from the previous response. Default: first page"
                },
                "privacy": {
                    "type": "string",
                    "description": "Filter models by privacy level.\nDefault: `private`. Values: `private`, `public`"
                },
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "type": {
                    "type": "string",
                    "description": "Filter models by type.\nExamples: `flux.1`, `flux.1-lora`, etc. Default: no filter"
                },
                "tags": {
                    "type": "string",
                    "description": "Filter models by tags. Multiple tags comma-separated.\nExample: `anime,portrait,style`. Default: no filter"
                },
                "excludeTypes": {
                    "type": "string",
                    "description": "Exclude models by type. Multiple types comma-separated. Example: `sd-1_5,flux.1`. Default: no exclusions"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-search-assets",
        "description": "Search for assets.\nAt least one of the following fields must have a value: `query`, `filter`, `image`, or `images`.\n\n`image`, `images` are mutually exclusive.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalAssets": {
                    "type": "string",
                    "description": "If set to true, returns the original asset without transformation"
                },
                "filter": {
                    "type": "string",
                    "description": "Filter queries by an attribute's value"
                },
                "image": {
                    "type": "string",
                    "description": "Search for similar images with `image` as a reference.\n\nMust be an existing `AssetId` or a valid data URL."
                },
                "imageSemanticRatio": {
                    "type": "number",
                    "description": "Image embedding ratio for hybrid search, applied when `image`, `images.like`, or `images.unlike`\nare provided"
                },
                "images": {
                    "type": "object",
                    "properties": {
                        "like": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "unlike": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        }
                    },
                    "required": []
                },
                "offset": {
                    "type": "number",
                    "description": "Number of documents to skip. Must be used with `limit`. Starts from 0."
                },
                "public": {
                    "type": "boolean",
                    "description": "Search for public images not necessarily belonging to the current `ownerId`"
                },
                "hitsPerPage": {
                    "type": "number",
                    "description": "Maximum number of documents returned for a page. Must be used with `page`."
                },
                "query": {
                    "type": "string",
                    "description": "A string used for querying search results."
                },
                "limit": {
                    "type": "number",
                    "description": "Maximum number of documents returned. Must be used with `offset`."
                },
                "sortBy": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "page": {
                    "type": "number",
                    "description": "Request a specific page of results. Must be used with `hitsPerPage`."
                },
                "querySemanticRatio": {
                    "type": "number",
                    "description": "Query embedding for hybrid search, if possible"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-search-models",
        "description": "Search for models.\nAt least one of the following fields must have a value: `query`, `filter`, `image`, or `images`.\n\n`image`, and `images` are mutually exclusive.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "originalModels": {
                    "type": "string"
                },
                "filter": {
                    "type": "string",
                    "description": "Filter queries by an attribute's value"
                },
                "image": {
                    "type": "string",
                    "description": "Search for model with `image` as a reference\n\nMust be an existing `AssetId` or a valid data URL."
                },
                "imageSemanticRatio": {
                    "type": "number",
                    "description": "Image embedding ratio for hybrid search, applied when `image`, `images.like`, or `images.unlike`\nare provided"
                },
                "images": {
                    "type": "object",
                    "properties": {
                        "like": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "unlike": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        }
                    },
                    "required": []
                },
                "offset": {
                    "type": "number",
                    "description": "Number of documents to skip. Must be used with `limit`. Starts from 0."
                },
                "public": {
                    "type": "boolean",
                    "description": "Search for public images not necessarily belonging to the current `ownerId`"
                },
                "hitsPerPage": {
                    "type": "number",
                    "description": "Maximum number of documents returned for a page. Must be used with `page`."
                },
                "query": {
                    "type": "string",
                    "description": "A string used for querying search results."
                },
                "limit": {
                    "type": "number",
                    "description": "Maximum number of documents returned. Must be used with `offset`."
                },
                "sortBy": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "page": {
                    "type": "number",
                    "description": "Request a specific page of results. Must be used with `hitsPerPage`."
                },
                "querySemanticRatio": {
                    "type": "number",
                    "description": "Query embedding for hybrid search, if possible"
                }
            },
            "required": []
        }
    },
    {
        "name": "get-tags",
        "description": "List all tags in use for the given `projectId`",
        "inputSchema": {
            "type": "object",
            "properties": {
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 50, maximum value is 100, minimum value is 1"
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-uploads",
        "description": "Create a temporary upload URL for a file. Support multipart uploads. Return a list of URLs for each part of the file.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "fileName": {
                    "type": "string",
                    "description": "Required for multipart upload. The original file name of the image (example: \"low-res-image.jpg\"). It will be ignored if assetId is provided."
                },
                "fileSize": {
                    "type": "number",
                    "description": "Required for multipart upload. The size of the file in bytes"
                },
                "kind": {
                    "type": "string",
                    "description": "Required for multipart upload and url. The purpose of the file once validated (example: \"model\")",
                    "enum": [
                        "3d",
                        "asset",
                        "audio",
                        "avatar",
                        "image",
                        "model",
                        "video"
                    ]
                },
                "civitaiModelUrl": {
                    "type": "string",
                    "description": "The civitai.com url of the model (example: \"https://civitai.com/models/370194/translucent-subsurface-scattering-test?modelVersionId=413566\")."
                },
                "huggingFaceModelName": {
                    "type": "string",
                    "description": "The huggingface.co modelName (example: \"stabilityai/stable-diffusion-xl-base-1.0\").\nNo need to setup other fields if you setup huggingFaceModelName"
                },
                "parts": {
                    "type": "number",
                    "description": "Required for multipart upload. The number of parts the file will be uploaded in"
                },
                "contentType": {
                    "type": "string",
                    "description": "Required for multipart upload. The MIME type of the file (example: \"image/jpeg\")"
                },
                "url": {
                    "type": "string",
                    "description": "The url where to download the file.\nIf you setup url you MUST setup kind as well."
                },
                "assetOptions": {
                    "type": "object",
                    "properties": {
                        "hide": {
                            "type": "boolean",
                            "description": "Specify if the asset should be hidden from the user."
                        },
                        "collectionIds": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "parentId": {
                            "type": "string",
                            "description": "The parentId of the asset."
                        }
                    },
                    "required": []
                }
            },
            "required": []
        }
    },
    {
        "name": "get-uploads",
        "description": "Get the details of an existing upload",
        "inputSchema": {
            "type": "object",
            "properties": {
                "uploadId": {
                    "type": "string",
                    "description": "The upload Id to retrieve"
                }
            },
            "required": [
                "uploadId"
            ]
        }
    },
    {
        "name": "post-uploads-action",
        "description": "Trigger an action on upload",
        "inputSchema": {
            "type": "object",
            "properties": {
                "uploadId": {
                    "type": "string",
                    "description": "The upload Id to retrieve"
                },
                "action": {
                    "type": "string",
                    "description": "The action to perform on an upload, currently only \"upload-complete\" is supported",
                    "enum": [
                        "complete"
                    ]
                }
            },
            "required": [
                "uploadId",
                "action"
            ]
        }
    },
    {
        "name": "get-usages",
        "description": "Provide usage data for the given filters. Such as consumed compute units, number of assets generated, etc. Maximum time range with custom startDate and endDate is 120 days. Granularity is calculated based on the time range.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "userId": {
                    "type": "string",
                    "description": "The unique identifier of the user for the usage. If not provided, returns all usages for the team."
                },
                "activityOffset": {
                    "type": "string",
                    "description": "The offset for the activity data. Default is 0. If bad offset or empty, 0 will be returned. Must be a positive integer."
                },
                "userIds": {
                    "type": "string",
                    "description": "The unique identifiers of the users for filtering the usage data. If not provided, use all users. Can be one or more comma separated values."
                },
                "type": {
                    "type": "string",
                    "description": "The type of the usage data. Can be one or more comma separated values. Default is all types. If bad type or empty, all types will be returned."
                },
                "endDate": {
                    "type": "string",
                    "description": "The end date of the usage in ISO 8601 format. If not provided, use default timeRange. If provided, startDate is required."
                },
                "projectIds": {
                    "type": "string",
                    "description": "The project ids for filtering the usage data. If not provided, use all projects. Can be one or more comma separated values."
                },
                "timeRange": {
                    "type": "string",
                    "description": "The time range of the usage. If not provided, use default timeRange. If startDate and endDate provided, timeRange is ignored."
                },
                "startDate": {
                    "type": "string",
                    "description": "The start date of the usage in ISO 8601 format. If not provided, use default timeRange. If provided, endDate is required."
                }
            },
            "required": []
        }
    },
    {
        "name": "get-workflows",
        "description": "List workflows",
        "inputSchema": {
            "type": "object",
            "properties": {
                "privacy": {
                    "type": "string"
                },
                "pageSize": {
                    "type": "string",
                    "description": "The number of items to return in the response. The default value is 10, maximum value is 200, minimum value is 1"
                },
                "paginationToken": {
                    "type": "string",
                    "description": "A token you received in a previous request to query the next page of items"
                }
            },
            "required": []
        }
    },
    {
        "name": "post-workflows",
        "description": "Create workflow",
        "inputSchema": {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "description"
            ]
        }
    },
    {
        "name": "get-workflows-by-workflow-id",
        "description": "Get workflow by ID",
        "inputSchema": {
            "type": "object",
            "properties": {
                "workflowId": {
                    "type": "string",
                    "description": "The workflow ID to retrieve"
                }
            },
            "required": [
                "workflowId"
            ]
        }
    },
    {
        "name": "put-workflows-by-workflow-id",
        "description": "Update workflow",
        "inputSchema": {
            "type": "object",
            "properties": {
                "workflowId": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                }
            },
            "required": [
                "workflowId"
            ]
        }
    },
    {
        "name": "delete-workflows-by-workflow-id",
        "description": "Delete workflow",
        "inputSchema": {
            "type": "object",
            "properties": {
                "workflowId": {
                    "type": "string",
                    "description": "The workflow ID to delete"
                }
            },
            "required": [
                "workflowId"
            ]
        }
    },
    {
        "name": "put-workflow-run-by-workflow-id",
        "description": "Run a workflow",
        "inputSchema": {
            "type": "object",
            "properties": {
                "workflowId": {
                    "type": "string"
                }
            },
            "required": [
                "workflowId"
            ]
        }
    }
];

export async function handleToolCall(name: string, args: any) {
    switch (name) {
        
        case "get-assets": {
            const url = `${BASE_URL}/assets`;
            const params: any = {};
            const data: any = {};
            
            if (args['updatedBefore'] !== undefined) params['updatedBefore'] = args['updatedBefore'];
if (args['sortDirection'] !== undefined) params['sortDirection'] = args['sortDirection'];
if (args['privacy'] !== undefined) params['privacy'] = args['privacy'];
if (args['inferenceId'] !== undefined) params['inferenceId'] = args['inferenceId'];
if (args['modelId'] !== undefined) params['modelId'] = args['modelId'];
if (args['updatedAfter'] !== undefined) params['updatedAfter'] = args['updatedAfter'];
if (args['parentAssetId'] !== undefined) params['parentAssetId'] = args['parentAssetId'];
if (args['createdBefore'] !== undefined) params['createdBefore'] = args['createdBefore'];
if (args['sortBy'] !== undefined) params['sortBy'] = args['sortBy'];
if (args['createdAfter'] !== undefined) params['createdAfter'] = args['createdAfter'];
if (args['authorId'] !== undefined) params['authorId'] = args['authorId'];
if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['rootAssetId'] !== undefined) params['rootAssetId'] = args['rootAssetId'];
if (args['type'] !== undefined) params['type'] = args['type'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];
if (args['tags'] !== undefined) params['tags'] = args['tags'];
if (args['types'] !== undefined) params['types'] = args['types'];
if (args['collectionId'] !== undefined) params['collectionId'] = args['collectionId'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-asset": {
            const url = `${BASE_URL}/assets`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['canvas'] !== undefined) data['canvas'] = args['canvas'];
if (args['thumbnail'] !== undefined) data['thumbnail'] = args['thumbnail'];
if (args['hide'] !== undefined) data['hide'] = args['hide'];
if (args['collectionIds'] !== undefined) data['collectionIds'] = args['collectionIds'];
if (args['name'] !== undefined) data['name'] = args['name'];
if (args['parentId'] !== undefined) data['parentId'] = args['parentId'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-asset": {
            const url = `${BASE_URL}/assets`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];

            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-download-assets": {
            const url = `${BASE_URL}/assets/download`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['query'] !== undefined) data['query'] = args['query'];
if (args['options'] !== undefined) data['options'] = args['options'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-download-assets": {
            const url = `${BASE_URL}/assets/download/${args.jobId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-asset-get-bulk": {
            const url = `${BASE_URL}/assets/get-bulk`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-public-assets": {
            const url = `${BASE_URL}/assets/public`;
            const params: any = {};
            const data: any = {};
            
            if (args['updatedBefore'] !== undefined) params['updatedBefore'] = args['updatedBefore'];
if (args['sortDirection'] !== undefined) params['sortDirection'] = args['sortDirection'];
if (args['modelId'] !== undefined) params['modelId'] = args['modelId'];
if (args['updatedAfter'] !== undefined) params['updatedAfter'] = args['updatedAfter'];
if (args['createdBefore'] !== undefined) params['createdBefore'] = args['createdBefore'];
if (args['sortBy'] !== undefined) params['sortBy'] = args['sortBy'];
if (args['createdAfter'] !== undefined) params['createdAfter'] = args['createdAfter'];
if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['type'] !== undefined) params['type'] = args['type'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];
if (args['tags'] !== undefined) params['tags'] = args['tags'];
if (args['types'] !== undefined) params['types'] = args['types'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-public-assets-by-asset-id": {
            const url = `${BASE_URL}/assets/public/${args.assetId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-assets-by-asset-id": {
            const url = `${BASE_URL}/assets/${args.assetId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['withEmbedding'] !== undefined) params['withEmbedding'] = args['withEmbedding'];
if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-asset-by-asset-id": {
            const url = `${BASE_URL}/assets/${args.assetId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['lockId'] !== undefined) data['lockId'] = args['lockId'];
if (args['lockExpiresAt'] !== undefined) data['lockExpiresAt'] = args['lockExpiresAt'];
if (args['canvas'] !== undefined) data['canvas'] = args['canvas'];
if (args['thumbnail'] !== undefined) data['thumbnail'] = args['thumbnail'];
if (args['name'] !== undefined) data['name'] = args['name'];
if (args['description'] !== undefined) data['description'] = args['description'];
if (args['disableSnapshot'] !== undefined) data['disableSnapshot'] = args['disableSnapshot'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "copy-asset-by-asset-id": {
            const url = `${BASE_URL}/assets/${args.assetId}/copy`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['targetProjectId'] !== undefined) data['targetProjectId'] = args['targetProjectId'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "lock-asset-by-asset-id": {
            const url = `${BASE_URL}/assets/${args.assetId}/lock`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['lockExpiresAt'] !== undefined) data['lockExpiresAt'] = args['lockExpiresAt'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-canvas-asset-snapshots": {
            const url = `${BASE_URL}/assets/${args.assetId}/snapshots`;
            const params: any = {};
            const data: any = {};
            
            if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-assets-tags-by-asset-id": {
            const url = `${BASE_URL}/assets/${args.assetId}/tags`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['add'] !== undefined) data['add'] = args['add'];
if (args['strict'] !== undefined) data['strict'] = args['strict'];
if (args['delete'] !== undefined) data['delete'] = args['delete'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "unlock-asset-by-asset-id": {
            const url = `${BASE_URL}/assets/${args.assetId}/unlock`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['lockId'] !== undefined) data['lockId'] = args['lockId'];
if (args['forceUnlock'] !== undefined) data['forceUnlock'] = args['forceUnlock'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-collections": {
            const url = `${BASE_URL}/collections`;
            const params: any = {};
            const data: any = {};
            
            if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-collection": {
            const url = `${BASE_URL}/collections`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['name'] !== undefined) data['name'] = args['name'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-collections-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-collections-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['name'] !== undefined) data['name'] = args['name'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-collections-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-assets-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}/assets`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-assets-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}/assets`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];

            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}/models`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['modelIds'] !== undefined) data['modelIds'] = args['modelIds'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-models-by-collection-id": {
            const url = `${BASE_URL}/collections/${args.collectionId}/models`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['modelIds'] !== undefined) data['modelIds'] = args['modelIds'];

            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-caption-inferences": {
            const url = `${BASE_URL}/generate/caption`;
            const params: any = {};
            const data: any = {};
            
            if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ensureIPCleared'] !== undefined) data['ensureIPCleared'] = args['ensureIPCleared'];
if (args['images'] !== undefined) data['images'] = args['images'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['unwantedSequences'] !== undefined) data['unwantedSequences'] = args['unwantedSequences'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['temperature'] !== undefined) data['temperature'] = args['temperature'];
if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];
if (args['topP'] !== undefined) data['topP'] = args['topP'];
if (args['detailsLevel'] !== undefined) data['detailsLevel'] = args['detailsLevel'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-controlnet-inferences": {
            const url = `${BASE_URL}/generate/controlnet`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['controlEnd'] !== undefined) data['controlEnd'] = args['controlEnd'];
if (args['modality'] !== undefined) data['modality'] = args['modality'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableModalityDetection'] !== undefined) data['disableModalityDetection'] = args['disableModalityDetection'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['controlStart'] !== undefined) data['controlStart'] = args['controlStart'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['controlImageId'] !== undefined) data['controlImageId'] = args['controlImageId'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['controlImage'] !== undefined) data['controlImage'] = args['controlImage'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-controlnet-img2img-inferences": {
            const url = `${BASE_URL}/generate/controlnet-img2img`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['controlEnd'] !== undefined) data['controlEnd'] = args['controlEnd'];
if (args['modality'] !== undefined) data['modality'] = args['modality'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableModalityDetection'] !== undefined) data['disableModalityDetection'] = args['disableModalityDetection'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['controlStart'] !== undefined) data['controlStart'] = args['controlStart'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['controlImageId'] !== undefined) data['controlImageId'] = args['controlImageId'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['controlImage'] !== undefined) data['controlImage'] = args['controlImage'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-controlnet-inpaint-inferences": {
            const url = `${BASE_URL}/generate/controlnet-inpaint`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['controlEnd'] !== undefined) data['controlEnd'] = args['controlEnd'];
if (args['modality'] !== undefined) data['modality'] = args['modality'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableMerging'] !== undefined) data['disableMerging'] = args['disableMerging'];
if (args['disableModalityDetection'] !== undefined) data['disableModalityDetection'] = args['disableModalityDetection'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['controlStart'] !== undefined) data['controlStart'] = args['controlStart'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];
if (args['controlImageId'] !== undefined) data['controlImageId'] = args['controlImageId'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['controlImage'] !== undefined) data['controlImage'] = args['controlImage'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-controlnet-inpaint-ip-adapter-inferences": {
            const url = `${BASE_URL}/generate/controlnet-inpaint-ip-adapter`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ipAdapterImageIds'] !== undefined) data['ipAdapterImageIds'] = args['ipAdapterImageIds'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['ipAdapterType'] !== undefined) data['ipAdapterType'] = args['ipAdapterType'];
if (args['ipAdapterImage'] !== undefined) data['ipAdapterImage'] = args['ipAdapterImage'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableMerging'] !== undefined) data['disableMerging'] = args['disableMerging'];
if (args['ipAdapterImages'] !== undefined) data['ipAdapterImages'] = args['ipAdapterImages'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];
if (args['controlImageId'] !== undefined) data['controlImageId'] = args['controlImageId'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['ipAdapterImageId'] !== undefined) data['ipAdapterImageId'] = args['ipAdapterImageId'];
if (args['ipAdapterScale'] !== undefined) data['ipAdapterScale'] = args['ipAdapterScale'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['ipAdapterScales'] !== undefined) data['ipAdapterScales'] = args['ipAdapterScales'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['controlImage'] !== undefined) data['controlImage'] = args['controlImage'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-controlnet-ip-adapter-inferences": {
            const url = `${BASE_URL}/generate/controlnet-ip-adapter`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ipAdapterImageIds'] !== undefined) data['ipAdapterImageIds'] = args['ipAdapterImageIds'];
if (args['controlEnd'] !== undefined) data['controlEnd'] = args['controlEnd'];
if (args['modality'] !== undefined) data['modality'] = args['modality'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['ipAdapterType'] !== undefined) data['ipAdapterType'] = args['ipAdapterType'];
if (args['ipAdapterImage'] !== undefined) data['ipAdapterImage'] = args['ipAdapterImage'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableModalityDetection'] !== undefined) data['disableModalityDetection'] = args['disableModalityDetection'];
if (args['ipAdapterImages'] !== undefined) data['ipAdapterImages'] = args['ipAdapterImages'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['controlStart'] !== undefined) data['controlStart'] = args['controlStart'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['controlImageId'] !== undefined) data['controlImageId'] = args['controlImageId'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['ipAdapterImageId'] !== undefined) data['ipAdapterImageId'] = args['ipAdapterImageId'];
if (args['ipAdapterScale'] !== undefined) data['ipAdapterScale'] = args['ipAdapterScale'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['ipAdapterScales'] !== undefined) data['ipAdapterScales'] = args['ipAdapterScales'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['controlImage'] !== undefined) data['controlImage'] = args['controlImage'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-controlnet-texture-inferences": {
            const url = `${BASE_URL}/generate/controlnet-texture`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['controlEnd'] !== undefined) data['controlEnd'] = args['controlEnd'];
if (args['modality'] !== undefined) data['modality'] = args['modality'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableModalityDetection'] !== undefined) data['disableModalityDetection'] = args['disableModalityDetection'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['controlStart'] !== undefined) data['controlStart'] = args['controlStart'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['controlImageId'] !== undefined) data['controlImageId'] = args['controlImageId'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['controlImage'] !== undefined) data['controlImage'] = args['controlImage'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-generate-custom": {
            const url = `${BASE_URL}/generate/custom/${args.modelId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args.body !== undefined) Object.assign(data, args.body);

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-describe-style-inferences": {
            const url = `${BASE_URL}/generate/describe-style`;
            const params: any = {};
            const data: any = {};
            
            if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ensureIPCleared'] !== undefined) data['ensureIPCleared'] = args['ensureIPCleared'];
if (args['images'] !== undefined) data['images'] = args['images'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['unwantedSequences'] !== undefined) data['unwantedSequences'] = args['unwantedSequences'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['temperature'] !== undefined) data['temperature'] = args['temperature'];
if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];
if (args['topP'] !== undefined) data['topP'] = args['topP'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-detect-inferences": {
            const url = `${BASE_URL}/generate/detect`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['modality'] !== undefined) data['modality'] = args['modality'];
if (args['lowThreshold'] !== undefined) data['lowThreshold'] = args['lowThreshold'];
if (args['removeBackground'] !== undefined) data['removeBackground'] = args['removeBackground'];
if (args['minThreshold'] !== undefined) data['minThreshold'] = args['minThreshold'];
if (args['maxThreshold'] !== undefined) data['maxThreshold'] = args['maxThreshold'];
if (args['factor'] !== undefined) data['factor'] = args['factor'];
if (args['highThreshold'] !== undefined) data['highThreshold'] = args['highThreshold'];
if (args['keypointThreshold'] !== undefined) data['keypointThreshold'] = args['keypointThreshold'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-embed-inferences": {
            const url = `${BASE_URL}/generate/embed`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['text'] !== undefined) data['text'] = args['text'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-generative-fill-inferences": {
            const url = `${BASE_URL}/generate/generative-fill`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['targetHeight'] !== undefined) data['targetHeight'] = args['targetHeight'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['promptFidelity'] !== undefined) data['promptFidelity'] = args['promptFidelity'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['targetWidth'] !== undefined) data['targetWidth'] = args['targetWidth'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-img2img-inferences": {
            const url = `${BASE_URL}/generate/img2img`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-img2img-ip-adapter-inferences": {
            const url = `${BASE_URL}/generate/img2img-ip-adapter`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ipAdapterImageIds'] !== undefined) data['ipAdapterImageIds'] = args['ipAdapterImageIds'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['ipAdapterType'] !== undefined) data['ipAdapterType'] = args['ipAdapterType'];
if (args['ipAdapterImage'] !== undefined) data['ipAdapterImage'] = args['ipAdapterImage'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['ipAdapterImages'] !== undefined) data['ipAdapterImages'] = args['ipAdapterImages'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['ipAdapterImageId'] !== undefined) data['ipAdapterImageId'] = args['ipAdapterImageId'];
if (args['ipAdapterScale'] !== undefined) data['ipAdapterScale'] = args['ipAdapterScale'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['ipAdapterScales'] !== undefined) data['ipAdapterScales'] = args['ipAdapterScales'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-img2img-texture-inferences": {
            const url = `${BASE_URL}/generate/img2img-texture`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-inpaint-inferences": {
            const url = `${BASE_URL}/generate/inpaint`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['disableMerging'] !== undefined) data['disableMerging'] = args['disableMerging'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-inpaint-ip-adapter-inferences": {
            const url = `${BASE_URL}/generate/inpaint-ip-adapter`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ipAdapterImageIds'] !== undefined) data['ipAdapterImageIds'] = args['ipAdapterImageIds'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['ipAdapterType'] !== undefined) data['ipAdapterType'] = args['ipAdapterType'];
if (args['ipAdapterImage'] !== undefined) data['ipAdapterImage'] = args['ipAdapterImage'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['disableMerging'] !== undefined) data['disableMerging'] = args['disableMerging'];
if (args['ipAdapterImages'] !== undefined) data['ipAdapterImages'] = args['ipAdapterImages'];
if (args['imageParentId'] !== undefined) data['imageParentId'] = args['imageParentId'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['height'] !== undefined) data['height'] = args['height'];
if (args['imageHide'] !== undefined) data['imageHide'] = args['imageHide'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageId'] !== undefined) data['imageId'] = args['imageId'];
if (args['ipAdapterImageId'] !== undefined) data['ipAdapterImageId'] = args['ipAdapterImageId'];
if (args['ipAdapterScale'] !== undefined) data['ipAdapterScale'] = args['ipAdapterScale'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['ipAdapterScales'] !== undefined) data['ipAdapterScales'] = args['ipAdapterScales'];
if (args['maskId'] !== undefined) data['maskId'] = args['maskId'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-patch-inferences": {
            const url = `${BASE_URL}/generate/patch`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['patch'] !== undefined) data['patch'] = args['patch'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['backgroundColor'] !== undefined) data['backgroundColor'] = args['backgroundColor'];
if (args['format'] !== undefined) data['format'] = args['format'];
if (args['position'] !== undefined) data['position'] = args['position'];
if (args['allowOverflow'] !== undefined) data['allowOverflow'] = args['allowOverflow'];
if (args['crop'] !== undefined) data['crop'] = args['crop'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-pixelate-inferences": {
            const url = `${BASE_URL}/generate/pixelate`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['pixelGridSize'] !== undefined) data['pixelGridSize'] = args['pixelGridSize'];
if (args['removeNoise'] !== undefined) data['removeNoise'] = args['removeNoise'];
if (args['removeBackground'] !== undefined) data['removeBackground'] = args['removeBackground'];
if (args['colorPalette'] !== undefined) data['colorPalette'] = args['colorPalette'];
if (args['colorPaletteSize'] !== undefined) data['colorPaletteSize'] = args['colorPaletteSize'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-prompt-inferences": {
            const url = `${BASE_URL}/generate/prompt`;
            const params: any = {};
            const data: any = {};
            
            if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['mode'] !== undefined) data['mode'] = args['mode'];
if (args['ensureIPCleared'] !== undefined) data['ensureIPCleared'] = args['ensureIPCleared'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['images'] !== undefined) data['images'] = args['images'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['temperature'] !== undefined) data['temperature'] = args['temperature'];
if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];
if (args['numResults'] !== undefined) data['numResults'] = args['numResults'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['topP'] !== undefined) data['topP'] = args['topP'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-prompt-editing-inferences": {
            const url = `${BASE_URL}/generate/prompt-editing`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['referenceImages'] !== undefined) data['referenceImages'] = args['referenceImages'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['guidanceScale'] !== undefined) data['guidanceScale'] = args['guidanceScale'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['format'] !== undefined) data['format'] = args['format'];
if (args['aspectRatio'] !== undefined) data['aspectRatio'] = args['aspectRatio'];
if (args['inputFidelity'] !== undefined) data['inputFidelity'] = args['inputFidelity'];
if (args['quality'] !== undefined) data['quality'] = args['quality'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['compression'] !== undefined) data['compression'] = args['compression'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['mask'] !== undefined) data['mask'] = args['mask'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-reframe-inferences": {
            const url = `${BASE_URL}/generate/reframe`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['inputLocation'] !== undefined) data['inputLocation'] = args['inputLocation'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['horizontalExpansionRatio'] !== undefined) data['horizontalExpansionRatio'] = args['horizontalExpansionRatio'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['resizeOption'] !== undefined) data['resizeOption'] = args['resizeOption'];
if (args['verticalExpansionRatio'] !== undefined) data['verticalExpansionRatio'] = args['verticalExpansionRatio'];
if (args['targetHeight'] !== undefined) data['targetHeight'] = args['targetHeight'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['promptFidelity'] !== undefined) data['promptFidelity'] = args['promptFidelity'];
if (args['overlapPercentage'] !== undefined) data['overlapPercentage'] = args['overlapPercentage'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['targetWidth'] !== undefined) data['targetWidth'] = args['targetWidth'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-remove-background-inferences": {
            const url = `${BASE_URL}/generate/remove-background`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['backgroundColor'] !== undefined) data['backgroundColor'] = args['backgroundColor'];
if (args['format'] !== undefined) data['format'] = args['format'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-restyle-inferences": {
            const url = `${BASE_URL}/generate/restyle`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['styleFidelity'] !== undefined) data['styleFidelity'] = args['styleFidelity'];
if (args['controlEnd'] !== undefined) data['controlEnd'] = args['controlEnd'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['styleImages'] !== undefined) data['styleImages'] = args['styleImages'];
if (args['promptFidelity'] !== undefined) data['promptFidelity'] = args['promptFidelity'];
if (args['clustering'] !== undefined) data['clustering'] = args['clustering'];
if (args['sketch'] !== undefined) data['sketch'] = args['sketch'];
if (args['structureFidelity'] !== undefined) data['structureFidelity'] = args['structureFidelity'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-segment-inferences": {
            const url = `${BASE_URL}/generate/segment`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['checkpoint'] !== undefined) data['checkpoint'] = args['checkpoint'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['resultContours'] !== undefined) data['resultContours'] = args['resultContours'];
if (args['dilate'] !== undefined) data['dilate'] = args['dilate'];
if (args['bbox'] !== undefined) data['bbox'] = args['bbox'];
if (args['backgroundOpacity'] !== undefined) data['backgroundOpacity'] = args['backgroundOpacity'];
if (args['betterQuality'] !== undefined) data['betterQuality'] = args['betterQuality'];
if (args['text'] !== undefined) data['text'] = args['text'];
if (args['resultImage'] !== undefined) data['resultImage'] = args['resultImage'];
if (args['points'] !== undefined) data['points'] = args['points'];
if (args['resultMask'] !== undefined) data['resultMask'] = args['resultMask'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-skybox-base360-inferences": {
            const url = `${BASE_URL}/generate/skybox-base-360`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['styleFidelity'] !== undefined) data['styleFidelity'] = args['styleFidelity'];
if (args['structureImage'] !== undefined) data['structureImage'] = args['structureImage'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['strength'] !== undefined) data['strength'] = args['strength'];
if (args['styleImages'] !== undefined) data['styleImages'] = args['styleImages'];
if (args['depthFidelity'] !== undefined) data['depthFidelity'] = args['depthFidelity'];
if (args['numOutputs'] !== undefined) data['numOutputs'] = args['numOutputs'];
if (args['depthImage'] !== undefined) data['depthImage'] = args['depthImage'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['overrideEmbeddings'] !== undefined) data['overrideEmbeddings'] = args['overrideEmbeddings'];
if (args['promptFidelity'] !== undefined) data['promptFidelity'] = args['promptFidelity'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['style'] !== undefined) data['style'] = args['style'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['structureFidelity'] !== undefined) data['structureFidelity'] = args['structureFidelity'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['cannyStructureImage'] !== undefined) data['cannyStructureImage'] = args['cannyStructureImage'];
if (args['geometryEnforcement'] !== undefined) data['geometryEnforcement'] = args['geometryEnforcement'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-skybox-upscale360-inferences": {
            const url = `${BASE_URL}/generate/skybox-upscale-360`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['styleFidelity'] !== undefined) data['styleFidelity'] = args['styleFidelity'];
if (args['sharpen'] !== undefined) data['sharpen'] = args['sharpen'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['styleImages'] !== undefined) data['styleImages'] = args['styleImages'];
if (args['scalingFactor'] !== undefined) data['scalingFactor'] = args['scalingFactor'];
if (args['detailsLevel'] !== undefined) data['detailsLevel'] = args['detailsLevel'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['overrideEmbeddings'] !== undefined) data['overrideEmbeddings'] = args['overrideEmbeddings'];
if (args['promptFidelity'] !== undefined) data['promptFidelity'] = args['promptFidelity'];
if (args['colorCorrection'] !== undefined) data['colorCorrection'] = args['colorCorrection'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['targetWidth'] !== undefined) data['targetWidth'] = args['targetWidth'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-texture-inferences": {
            const url = `${BASE_URL}/generate/texture`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['defaultParameters'] !== undefined) data['defaultParameters'] = args['defaultParameters'];
if (args['polished'] !== undefined) data['polished'] = args['polished'];
if (args['angular'] !== undefined) data['angular'] = args['angular'];
if (args['invert'] !== undefined) data['invert'] = args['invert'];
if (args['saveFlipbook'] !== undefined) data['saveFlipbook'] = args['saveFlipbook'];
if (args['texture'] !== undefined) data['texture'] = args['texture'];
if (args['raised'] !== undefined) data['raised'] = args['raised'];
if (args['shiny'] !== undefined) data['shiny'] = args['shiny'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-translate-inferences": {
            const url = `${BASE_URL}/generate/translate`;
            const params: any = {};
            const data: any = {};
            
            if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ensureIPCleared'] !== undefined) data['ensureIPCleared'] = args['ensureIPCleared'];
if (args['images'] !== undefined) data['images'] = args['images'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['temperature'] !== undefined) data['temperature'] = args['temperature'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['topP'] !== undefined) data['topP'] = args['topP'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-txt2img-inferences": {
            const url = `${BASE_URL}/generate/txt2img`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['aspectRatio'] !== undefined) data['aspectRatio'] = args['aspectRatio'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['height'] !== undefined) data['height'] = args['height'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-txt2img-ip-adapter-inferences": {
            const url = `${BASE_URL}/generate/txt2img-ip-adapter`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['ipAdapterImageIds'] !== undefined) data['ipAdapterImageIds'] = args['ipAdapterImageIds'];
if (args['ipAdapterImageId'] !== undefined) data['ipAdapterImageId'] = args['ipAdapterImageId'];
if (args['ipAdapterScale'] !== undefined) data['ipAdapterScale'] = args['ipAdapterScale'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['ipAdapterType'] !== undefined) data['ipAdapterType'] = args['ipAdapterType'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['aspectRatio'] !== undefined) data['aspectRatio'] = args['aspectRatio'];
if (args['ipAdapterScales'] !== undefined) data['ipAdapterScales'] = args['ipAdapterScales'];
if (args['ipAdapterImage'] !== undefined) data['ipAdapterImage'] = args['ipAdapterImage'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['ipAdapterImages'] !== undefined) data['ipAdapterImages'] = args['ipAdapterImages'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['height'] !== undefined) data['height'] = args['height'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-txt2img-texture-inferences": {
            const url = `${BASE_URL}/generate/txt2img-texture`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['modelId'] !== undefined) data['modelId'] = args['modelId'];
if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];
if (args['hideResults'] !== undefined) data['hideResults'] = args['hideResults'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['scheduler'] !== undefined) data['scheduler'] = args['scheduler'];
if (args['intermediateImages'] !== undefined) data['intermediateImages'] = args['intermediateImages'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['guidance'] !== undefined) data['guidance'] = args['guidance'];
if (args['numInferenceSteps'] !== undefined) data['numInferenceSteps'] = args['numInferenceSteps'];
if (args['numSamples'] !== undefined) data['numSamples'] = args['numSamples'];
if (args['width'] !== undefined) data['width'] = args['width'];
if (args['negativePromptStrength'] !== undefined) data['negativePromptStrength'] = args['negativePromptStrength'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['height'] !== undefined) data['height'] = args['height'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-upscale-inferences": {
            const url = `${BASE_URL}/generate/upscale`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['image'] !== undefined) data['image'] = args['image'];
if (args['creativityDecay'] !== undefined) data['creativityDecay'] = args['creativityDecay'];
if (args['tileStyle'] !== undefined) data['tileStyle'] = args['tileStyle'];
if (args['seed'] !== undefined) data['seed'] = args['seed'];
if (args['styleImages'] !== undefined) data['styleImages'] = args['styleImages'];
if (args['scalingFactor'] !== undefined) data['scalingFactor'] = args['scalingFactor'];
if (args['preset'] !== undefined) data['preset'] = args['preset'];
if (args['styleImagesFidelity'] !== undefined) data['styleImagesFidelity'] = args['styleImagesFidelity'];
if (args['detailsLevel'] !== undefined) data['detailsLevel'] = args['detailsLevel'];
if (args['fractality'] !== undefined) data['fractality'] = args['fractality'];
if (args['negativePrompt'] !== undefined) data['negativePrompt'] = args['negativePrompt'];
if (args['overrideEmbeddings'] !== undefined) data['overrideEmbeddings'] = args['overrideEmbeddings'];
if (args['promptFidelity'] !== undefined) data['promptFidelity'] = args['promptFidelity'];
if (args['style'] !== undefined) data['style'] = args['style'];
if (args['refinementSteps'] !== undefined) data['refinementSteps'] = args['refinementSteps'];
if (args['creativity'] !== undefined) data['creativity'] = args['creativity'];
if (args['imageFidelity'] !== undefined) data['imageFidelity'] = args['imageFidelity'];
if (args['imageType'] !== undefined) data['imageType'] = args['imageType'];
if (args['prompt'] !== undefined) data['prompt'] = args['prompt'];
if (args['targetWidth'] !== undefined) data['targetWidth'] = args['targetWidth'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-vectorize-inferences": {
            const url = `${BASE_URL}/generate/vectorize`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['mode'] !== undefined) data['mode'] = args['mode'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['layerDifference'] !== undefined) data['layerDifference'] = args['layerDifference'];
if (args['maxIterations'] !== undefined) data['maxIterations'] = args['maxIterations'];
if (args['cornerThreshold'] !== undefined) data['cornerThreshold'] = args['cornerThreshold'];
if (args['colorPrecision'] !== undefined) data['colorPrecision'] = args['colorPrecision'];
if (args['spliceThreshold'] !== undefined) data['spliceThreshold'] = args['spliceThreshold'];
if (args['lengthThreshold'] !== undefined) data['lengthThreshold'] = args['lengthThreshold'];
if (args['colorMode'] !== undefined) data['colorMode'] = args['colorMode'];
if (args['filterSpeckle'] !== undefined) data['filterSpeckle'] = args['filterSpeckle'];
if (args['preset'] !== undefined) data['preset'] = args['preset'];
if (args['pathPrecision'] !== undefined) data['pathPrecision'] = args['pathPrecision'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-jobs": {
            const url = `${BASE_URL}/jobs`;
            const params: any = {};
            const data: any = {};
            
            if (args['authorId'] !== undefined) params['authorId'] = args['authorId'];
if (args['hideResults'] !== undefined) params['hideResults'] = args['hideResults'];
if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['type'] !== undefined) params['type'] = args['type'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];
if (args['types'] !== undefined) params['types'] = args['types'];
if (args['status'] !== undefined) params['status'] = args['status'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-job-id": {
            const url = `${BASE_URL}/jobs/${args.jobId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-job-action-by-job-id": {
            const url = `${BASE_URL}/jobs/${args.jobId}/action`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['action'] !== undefined) data['action'] = args['action'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-models": {
            const url = `${BASE_URL}/models`;
            const params: any = {};
            const data: any = {};
            
            if (args['privacy'] !== undefined) params['privacy'] = args['privacy'];
if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];
if (args['blacklisted'] !== undefined) params['blacklisted'] = args['blacklisted'];
if (args['status'] !== undefined) params['status'] = args['status'];
if (args['collectionId'] !== undefined) params['collectionId'] = args['collectionId'];
if (args['loadedOnly'] !== undefined) params['loadedOnly'] = args['loadedOnly'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-models": {
            const url = `${BASE_URL}/models`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['collectionIds'] !== undefined) data['collectionIds'] = args['collectionIds'];
if (args['classSlug'] !== undefined) data['classSlug'] = args['classSlug'];
if (args['name'] !== undefined) data['name'] = args['name'];
if (args['shortDescription'] !== undefined) data['shortDescription'] = args['shortDescription'];
if (args['baseModelId'] !== undefined) data['baseModelId'] = args['baseModelId'];
if (args['type'] !== undefined) data['type'] = args['type'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-models-get-bulk": {
            const url = `${BASE_URL}/models/get-bulk`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['settings'] !== undefined) data['settings'] = args['settings'];
if (args['trainingImagesPreview'] !== undefined) data['trainingImagesPreview'] = args['trainingImagesPreview'];
if (args['minimal'] !== undefined) data['minimal'] = args['minimal'];
if (args['thumbnail'] !== undefined) data['thumbnail'] = args['thumbnail'];
if (args['allTrainingImages'] !== undefined) data['allTrainingImages'] = args['allTrainingImages'];
if (args['modelIds'] !== undefined) data['modelIds'] = args['modelIds'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-public-models": {
            const url = `${BASE_URL}/models/public`;
            const params: any = {};
            const data: any = {};
            
            if (args['updatedBefore'] !== undefined) params['updatedBefore'] = args['updatedBefore'];
if (args['sortDirection'] !== undefined) params['sortDirection'] = args['sortDirection'];
if (args['collectionIds'] !== undefined) params['collectionIds'] = args['collectionIds'];
if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['type'] !== undefined) params['type'] = args['type'];
if (args['updatedAfter'] !== undefined) params['updatedAfter'] = args['updatedAfter'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];
if (args['tags'] !== undefined) params['tags'] = args['tags'];
if (args['types'] !== undefined) params['types'] = args['types'];
if (args['createdBefore'] !== undefined) params['createdBefore'] = args['createdBefore'];
if (args['sortBy'] !== undefined) params['sortBy'] = args['sortBy'];
if (args['createdAfter'] !== undefined) params['createdAfter'] = args['createdAfter'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-public-models-by-model-id": {
            const url = `${BASE_URL}/models/public/${args.modelId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-models-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['negativePromptEmbedding'] !== undefined) data['negativePromptEmbedding'] = args['negativePromptEmbedding'];
if (args['thumbnail'] !== undefined) data['thumbnail'] = args['thumbnail'];
if (args['concepts'] !== undefined) data['concepts'] = args['concepts'];
if (args['classSlug'] !== undefined) data['classSlug'] = args['classSlug'];
if (args['name'] !== undefined) data['name'] = args['name'];
if (args['epoch'] !== undefined) data['epoch'] = args['epoch'];
if (args['promptEmbedding'] !== undefined) data['promptEmbedding'] = args['promptEmbedding'];
if (args['shortDescription'] !== undefined) data['shortDescription'] = args['shortDescription'];
if (args['type'] !== undefined) data['type'] = args['type'];
if (args['parameters'] !== undefined) data['parameters'] = args['parameters'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-models-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-models-copy-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/copy`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['copyExamples'] !== undefined) data['copyExamples'] = args['copyExamples'];
if (args['copyAsTrained'] !== undefined) data['copyAsTrained'] = args['copyAsTrained'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-models-description-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/description`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-description-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/description`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['description'] !== undefined) data['description'] = args['description'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-download-model": {
            const url = `${BASE_URL}/models/${args.modelId}/download`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['modelEpoch'] !== undefined) data['modelEpoch'] = args['modelEpoch'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-models-examples-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/examples`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-examples-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/examples`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-models-images-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/images`;
            const params: any = {};
            const data: any = {};
            
            if (args['ids'] !== undefined) params['ids'] = args['ids'];

            
            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-model-presets-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/presets`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-model-preset-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/presets`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['isDefault'] !== undefined) data['isDefault'] = args['isDefault'];
if (args['inferenceId'] !== undefined) data['inferenceId'] = args['inferenceId'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-model-preset-by-model-id-and-preset-id": {
            const url = `${BASE_URL}/models/${args.modelId}/presets/${args.presetId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['isDefault'] !== undefined) data['isDefault'] = args['isDefault'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-model-preset-by-model-id-and-preset-id": {
            const url = `${BASE_URL}/models/${args.modelId}/presets/${args.presetId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-models-scores-prompt-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/scores/prompt`;
            const params: any = {};
            const data: any = {};
            
            if (args['prompt'] !== undefined) params['prompt'] = args['prompt'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-models-scores-training-dataset-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/scores/training-dataset`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-tags-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/tags`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['add'] !== undefined) data['add'] = args['add'];
if (args['strict'] !== undefined) data['strict'] = args['strict'];
if (args['delete'] !== undefined) data['delete'] = args['delete'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-train-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/train`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['dryRun'] !== undefined) params['dryRun'] = args['dryRun'];

            if (args['parameters'] !== undefined) data['parameters'] = args['parameters'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-model-training-action-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/train/action`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['action'] !== undefined) data['action'] = args['action'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-models-training-images-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/training-images`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['data'] !== undefined) data['data'] = args['data'];
if (args['assetId'] !== undefined) data['assetId'] = args['assetId'];
if (args['name'] !== undefined) data['name'] = args['name'];
if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];
if (args['preset'] !== undefined) data['preset'] = args['preset'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-training-images-pairs-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/training-images/pairs`;
            const params: any = {};
            const data: any = {};
            
            
            if (args.body !== undefined) Object.assign(data, args.body);

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-models-training-images-by-model-id-and-training-image-id": {
            const url = `${BASE_URL}/models/${args.modelId}/training-images/${args.trainingImageId}`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['data'] !== undefined) data['data'] = args['data'];
if (args['assetId'] !== undefined) data['assetId'] = args['assetId'];
if (args['name'] !== undefined) data['name'] = args['name'];
if (args['assetIds'] !== undefined) data['assetIds'] = args['assetIds'];
if (args['preset'] !== undefined) data['preset'] = args['preset'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-models-training-images-by-model-id-and-training-image-id": {
            const url = `${BASE_URL}/models/${args.modelId}/training-images/${args.trainingImageId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-models-transfer-by-model-id": {
            const url = `${BASE_URL}/models/${args.modelId}/transfer`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['destinationProjectId'] !== undefined) data['destinationProjectId'] = args['destinationProjectId'];
if (args['destinationTeamId'] !== undefined) data['destinationTeamId'] = args['destinationTeamId'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-public-oscu-prices": {
            const url = `${BASE_URL}/oscu/prices`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-recommendations-models": {
            const url = `${BASE_URL}/recommendations/models`;
            const params: any = {};
            const data: any = {};
            
            if (args['capabilities'] !== undefined) params['capabilities'] = args['capabilities'];
if (args['excludeModelIds'] !== undefined) params['excludeModelIds'] = args['excludeModelIds'];
if (args['limit'] !== undefined) params['limit'] = args['limit'];
if (args['nextToken'] !== undefined) params['nextToken'] = args['nextToken'];
if (args['privacy'] !== undefined) params['privacy'] = args['privacy'];
if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];
if (args['type'] !== undefined) params['type'] = args['type'];
if (args['tags'] !== undefined) params['tags'] = args['tags'];
if (args['excludeTypes'] !== undefined) params['excludeTypes'] = args['excludeTypes'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-search-assets": {
            const url = `${BASE_URL}/search/assets`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalAssets'] !== undefined) params['originalAssets'] = args['originalAssets'];

            if (args['filter'] !== undefined) data['filter'] = args['filter'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageSemanticRatio'] !== undefined) data['imageSemanticRatio'] = args['imageSemanticRatio'];
if (args['images'] !== undefined) data['images'] = args['images'];
if (args['offset'] !== undefined) data['offset'] = args['offset'];
if (args['public'] !== undefined) data['public'] = args['public'];
if (args['hitsPerPage'] !== undefined) data['hitsPerPage'] = args['hitsPerPage'];
if (args['query'] !== undefined) data['query'] = args['query'];
if (args['limit'] !== undefined) data['limit'] = args['limit'];
if (args['sortBy'] !== undefined) data['sortBy'] = args['sortBy'];
if (args['page'] !== undefined) data['page'] = args['page'];
if (args['querySemanticRatio'] !== undefined) data['querySemanticRatio'] = args['querySemanticRatio'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-search-models": {
            const url = `${BASE_URL}/search/models`;
            const params: any = {};
            const data: any = {};
            
            if (args['originalModels'] !== undefined) params['originalModels'] = args['originalModels'];

            if (args['filter'] !== undefined) data['filter'] = args['filter'];
if (args['image'] !== undefined) data['image'] = args['image'];
if (args['imageSemanticRatio'] !== undefined) data['imageSemanticRatio'] = args['imageSemanticRatio'];
if (args['images'] !== undefined) data['images'] = args['images'];
if (args['offset'] !== undefined) data['offset'] = args['offset'];
if (args['public'] !== undefined) data['public'] = args['public'];
if (args['hitsPerPage'] !== undefined) data['hitsPerPage'] = args['hitsPerPage'];
if (args['query'] !== undefined) data['query'] = args['query'];
if (args['limit'] !== undefined) data['limit'] = args['limit'];
if (args['sortBy'] !== undefined) data['sortBy'] = args['sortBy'];
if (args['page'] !== undefined) data['page'] = args['page'];
if (args['querySemanticRatio'] !== undefined) data['querySemanticRatio'] = args['querySemanticRatio'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-tags": {
            const url = `${BASE_URL}/tags`;
            const params: any = {};
            const data: any = {};
            
            if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-uploads": {
            const url = `${BASE_URL}/uploads`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['fileName'] !== undefined) data['fileName'] = args['fileName'];
if (args['fileSize'] !== undefined) data['fileSize'] = args['fileSize'];
if (args['kind'] !== undefined) data['kind'] = args['kind'];
if (args['civitaiModelUrl'] !== undefined) data['civitaiModelUrl'] = args['civitaiModelUrl'];
if (args['huggingFaceModelName'] !== undefined) data['huggingFaceModelName'] = args['huggingFaceModelName'];
if (args['parts'] !== undefined) data['parts'] = args['parts'];
if (args['contentType'] !== undefined) data['contentType'] = args['contentType'];
if (args['url'] !== undefined) data['url'] = args['url'];
if (args['assetOptions'] !== undefined) data['assetOptions'] = args['assetOptions'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-uploads": {
            const url = `${BASE_URL}/uploads/${args.uploadId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-uploads-action": {
            const url = `${BASE_URL}/uploads/${args.uploadId}/action`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['action'] !== undefined) data['action'] = args['action'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-usages": {
            const url = `${BASE_URL}/usages`;
            const params: any = {};
            const data: any = {};
            
            if (args['userId'] !== undefined) params['userId'] = args['userId'];
if (args['activityOffset'] !== undefined) params['activityOffset'] = args['activityOffset'];
if (args['userIds'] !== undefined) params['userIds'] = args['userIds'];
if (args['type'] !== undefined) params['type'] = args['type'];
if (args['endDate'] !== undefined) params['endDate'] = args['endDate'];
if (args['projectIds'] !== undefined) params['projectIds'] = args['projectIds'];
if (args['timeRange'] !== undefined) params['timeRange'] = args['timeRange'];
if (args['startDate'] !== undefined) params['startDate'] = args['startDate'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-workflows": {
            const url = `${BASE_URL}/workflows`;
            const params: any = {};
            const data: any = {};
            
            if (args['privacy'] !== undefined) params['privacy'] = args['privacy'];
if (args['pageSize'] !== undefined) params['pageSize'] = args['pageSize'];
if (args['paginationToken'] !== undefined) params['paginationToken'] = args['paginationToken'];

            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "post-workflows": {
            const url = `${BASE_URL}/workflows`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['name'] !== undefined) data['name'] = args['name'];
if (args['description'] !== undefined) data['description'] = args['description'];

            
            const response = await axios({
                method: "post",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "get-workflows-by-workflow-id": {
            const url = `${BASE_URL}/workflows/${args.workflowId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "get",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-workflows-by-workflow-id": {
            const url = `${BASE_URL}/workflows/${args.workflowId}`;
            const params: any = {};
            const data: any = {};
            
            
            if (args['name'] !== undefined) data['name'] = args['name'];
if (args['description'] !== undefined) data['description'] = args['description'];

            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "delete-workflows-by-workflow-id": {
            const url = `${BASE_URL}/workflows/${args.workflowId}`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "delete",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }

        case "put-workflow-run-by-workflow-id": {
            const url = `${BASE_URL}/workflows/${args.workflowId}/run`;
            const params: any = {};
            const data: any = {};
            
            
            
            
            const response = await axios({
                method: "put",
                url: url,
                headers: getAuthHeaders(),
                params: params,
                data: data
            });
            return {
                content: [{ type: "text", text: JSON.stringify(response.data, null, 2) }]
            };
        }
        default:
            throw new Error(`Unknown tool: ${name}`);
    }
}
